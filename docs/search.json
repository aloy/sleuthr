[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Manual for Stat 230",
    "section": "",
    "text": "Overview\nThis is an R Manual for Adam Loy’s section of Applied Regression Analysis (Stat 230) taught in Fall 2025.\nThis document is intended to help describe how to undertake analyses introduced as examples in the Third Edition of the Statistical Sleuth (2013) by Fred Ramsey and Dan Schafer. More information about the book can be found at http://www.proaxis.com/~panorama/home.htm.\nThis work adapts work done by Linda Loi, Ruobing Zhang, Kate Aloisio, and Nicholas J. Horton. Their work leveraged initiatives undertaken by Project MOSAIC (http://www.mosaic-web.org), an NSF-funded effort to improve the teaching of statistics, calculus, science and computing in the undergraduate curriculum.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#textbook",
    "href": "index.html#textbook",
    "title": "R Manual for Stat 230",
    "section": "Textbook",
    "text": "Textbook\nThe Statistical Sleuth, third edition by Ramsey and Schaffer",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#data-sets",
    "href": "index.html#data-sets",
    "title": "R Manual for Stat 230",
    "section": "Data sets",
    "text": "Data sets\nThe data sets provided by the textbook authors an be accessed through the {Sleuth3} R package. Once you have installed the package (which is already on our server), you can access the data sets using the data() command:\n\n# Example loading the data set for Case study 1 in Chapter 1\ndata(\"case0101\", package = \"Sleuth3\")",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#r-packages",
    "href": "index.html#r-packages",
    "title": "R Manual for Stat 230",
    "section": "R packages",
    "text": "R packages\nMy goal is to teach you a realistic set of R packages for statistical modeling that are powerful, yet have low overhead. To this end, we will use numerous add-on packages throughout the term.\n\n\n\n\n\n\nWarning\n\n\n\nThe list below will be updated throughout the term.\n\n\n\n# Runnning list of R packages\nlibrary(ggformula) # data viz\nlibrary(mosaic)    # summary stats, etc.\nlibrary(dplyr)     # data wrangling\nlibrary(car)       # convenient functions for regression modeling\nlibrary(broom)     # formatting/extracting regression output\n\nAll of these pacakges have already been installed on maize.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "chapter01.html",
    "href": "chapter01.html",
    "title": "1  Drawing Statistical Conclusions",
    "section": "",
    "text": "2 Introduction\nTo use a package within R, it must be installed (one time), and loaded (each session). The package can be installed using the following command:\ninstall.packages(\"dplyr\")               # note the quotation marks\nOnce this is installed, it can be loaded by running the command:\nlibrary(dplyr)\nThis needs to be done once per session.\nThe dplyr package will allow us to easily manipulate data. We will also use the ggformula package for data visualization:\ninstall.packages(\"ggformula\")               # note the quotation marks\nlibrary(ggformula)\nIn addition the data files for the Sleuth case studies can be accessed by installing the Sleuth3 package.\ninstall.packages(\"Sleuth3\")               # note the quotation marks\nlibrary(Sleuth3)\nWe also set some options to improve legibility of graphs and output.\noptions(digits=3) # display three significant digits by default\nThe specific goal of this document is to demonstrate how to calculate the quantities described in Chapter 1: Drawing Statistical Conclusions using R.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Drawing Statistical Conclusions</span>"
    ]
  },
  {
    "objectID": "chapter01.html#statistical-summary-and-graphical-display",
    "href": "chapter01.html#statistical-summary-and-graphical-display",
    "title": "1  Drawing Statistical Conclusions",
    "section": "3.1 Statistical summary and graphical display",
    "text": "3.1 Statistical summary and graphical display\nWe begin by reading the data and summarizing the variables.\n\ndata(case0101)\nsummary(case0101)\n\n     Score          Treatment \n Min.   : 5.0   Extrinsic:23  \n 1st Qu.:14.9   Intrinsic:24  \n Median :18.7                 \n Mean   :17.9                 \n 3rd Qu.:21.2                 \n Max.   :29.7                 \n\n\nA total of 47 subjects with considerable experience in creative writing were randomly assigned to one of two treatment groups: 23 were placed into the “extrinsic” treatment group and 24 were placed into the “intrinsic” treatment group, as summarized in Display 1.1 (Sleuth, page 2)\nTo calculate summary statistics for each group we can use the tools in the dplyr package:\n\ncase0101 %&gt;%\n  group_by(Treatment) %&gt;%\n  summarize(min = min(Score), Q1 = quantile(Score, probs = .25),\n            median = median(Score), Q3 = quantile(Score, probs = .75),\n            max = max(Score), mean = mean(Score), sd = sd(Score))\n\n# A tibble: 2 × 8\n  Treatment   min    Q1 median    Q3   max  mean    sd\n  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Extrinsic     5  12.2   17.2  19.0  24    15.7  5.25\n2 Intrinsic    12  17.4   20.4  22.3  29.7  19.9  4.44\n\n\nAlternatively, you can use tapply (which may be familiar from a pervious class):\n\ntapply(case0101$Score, case0101$Treatment, summary) # 5-number summary + mean\n\n$Extrinsic\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    5.0    12.2    17.2    15.7    18.9    24.0 \n\n$Intrinsic\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    17.4    20.4    19.9    22.3    29.7 \n\ntapply(case0101$Score, case0101$Treatment, sd)      # std. deviation\n\nExtrinsic Intrinsic \n     5.25      4.44 \n\n\n\ngf_histogram( ~Score | Treatment, data = case0101, binwidth = 5)\n\n\n\n\n\n\n\n\nTo create stem-and-leaf plots for each level of a categorical variable, first load the CarletonStats package\n\nlibrary(CarletonStats)\nstemPlot(Score ~ Treatment, data = case0101)\n\n\n***Stem and Leaf plot for  Score ***\n   Grouped by levels of  Treatment \n\n    Extrinsic \n :\n  The decimal point is 1 digit(s) to the right of the |\n\n  0 | 556\n  1 | 1222\n  1 | 55777789999\n  2 | 01124\n\n    Intrinsic \n :\n  The decimal point is 1 digit(s) to the right of the |\n\n  1 | 2234\n  1 | 778899\n  2 | 001112223344\n  2 | 7\n  3 | 0\n\n\nThe extrinsic group (\\(n=23\\)) has an average creativity score that is 4.1 points less than the intrinsic group (\\(n=24\\)). The extrinsic group has relatively larger spread than the intrinsic group (\\(\\text{sd}=5.25\\) for extrinsic group and \\(\\text{sd}=4.44\\) for intrinsic group). Both distributions are approximately normally distributed.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Drawing Statistical Conclusions</span>"
    ]
  },
  {
    "objectID": "chapter01.html#inferential-procedures-two-sample-t-test",
    "href": "chapter01.html#inferential-procedures-two-sample-t-test",
    "title": "1  Drawing Statistical Conclusions",
    "section": "3.2 Inferential procedures (two-sample t-test)",
    "text": "3.2 Inferential procedures (two-sample t-test)\n\nt.test(Score ~ Treatment, alternative = \"two.sided\", data = case0101)\n\n\n    Welch Two Sample t-test\n\ndata:  Score by Treatment\nt = -3, df = 43, p-value = 0.006\nalternative hypothesis: true difference in means between group Extrinsic and group Intrinsic is not equal to 0\n95 percent confidence interval:\n -7.01 -1.28\nsample estimates:\nmean in group Extrinsic mean in group Intrinsic \n                   15.7                    19.9 \n\n\nThe two-sample t-test shows strong evidence that a subject would receive a lower creativity score for a poem written after the extrinsic motivation questionnaire than for one written after the intrinsic motivation questionnaire. The two-sided p-value is 0.006, which is small enough to reject the null hypothesis.\nThus, we can conclude that there is a difference between the population mean in the extrinsic group and the population mean in the intrinsic group; the estimated difference between these two scores is 4.1 points on the 0-40 point scale. A 95% confidence interval for the decrease in score due to having extrinsic motivation rather than intrinsic motivation is between \\(-1.28\\) and \\(-7.01\\) points (Sleuth, page 3).\nIn the creativity study, the question of whether there is a treatment effect becomes a question of whether the parameter has a nonzero value. The value of the test statistic for the creativity scores is 4.14. (Sleuth, page 11).\n\nsummary(lm(Score ~ Treatment, data = case0101))\n\n\nCall:\nlm(formula = Score ~ Treatment, data = case0101)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-10.74  -2.98   1.06   2.96   9.82 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           15.74       1.01   15.55   &lt;2e-16 ***\nTreatmentIntrinsic     4.14       1.42    2.93   0.0054 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.85 on 45 degrees of freedom\nMultiple R-squared:  0.16,  Adjusted R-squared:  0.141 \nF-statistic: 8.56 on 1 and 45 DF,  p-value: 0.00537",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Drawing Statistical Conclusions</span>"
    ]
  },
  {
    "objectID": "chapter01.html#permutation-test",
    "href": "chapter01.html#permutation-test",
    "title": "1  Drawing Statistical Conclusions",
    "section": "3.3 Permutation test",
    "text": "3.3 Permutation test\n\n# permTest is in the CarletonStats package\npermTest(Score ~ Treatment, data = case0101, alternative = \"two.sided\", B = 1000)\n\n\n    ** Permutation test **\n\n Permutation test with alternative: two.sided \n Observed statistic\n  Extrinsic :  15.7      Intrinsic :  19.9 \n Observed difference: -4.14 \n\n Mean of permutation distribution: 0.00461 \n Standard error of permutation distribution: 1.52 \n P-value:  0.005 \n\n    *-------------*\n\n\n\n\n\n\n\n\n\nAs described in the Sleuth on page 12, if the group assignment changes, we will get different results. First, the test statistics will be just as likely to be negative as positive. Second, the majority of values fall in the range from \\(-3.0\\) to \\(+3.0\\). Third, only few of the 1,000 randomization produced test statistics as large as 4.14. This last point indicates that 4.14 is a value corresponding to an unusually uneven randomization outcome, if the null hypothesis is correct.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Drawing Statistical Conclusions</span>"
    ]
  },
  {
    "objectID": "chapter01.html#inferential-procedures-two-sample-t-test-1",
    "href": "chapter01.html#inferential-procedures-two-sample-t-test-1",
    "title": "1  Drawing Statistical Conclusions",
    "section": "4.1 Inferential procedures (two-sample t-test)",
    "text": "4.1 Inferential procedures (two-sample t-test)\nThe t-test on page 4 of Sleuth can be replicated using the following commands (note that the equal-variance t-test is specified by var.equal=TRUE which is not the default).\n\nt.test(Salary ~ Sex, var.equal = TRUE, data = case0102, alternative = \"less\")\n\n\n    Two Sample t-test\n\ndata:  Salary by Sex\nt = -6, df = 91, p-value = 5e-09\nalternative hypothesis: true difference in means between group Female and group Male is less than 0\n95 percent confidence interval:\n -Inf -602\nsample estimates:\nmean in group Female   mean in group Male \n                5139                 5957",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Drawing Statistical Conclusions</span>"
    ]
  },
  {
    "objectID": "chapter01.html#permutation-test-1",
    "href": "chapter01.html#permutation-test-1",
    "title": "1  Drawing Statistical Conclusions",
    "section": "4.2 Permutation test",
    "text": "4.2 Permutation test\nWe undertake a permutation test to assess whether the differences in the center of these samples that we are observing are due to chance, if the distributions are actually equivalent back in the populations of male and female possible clerical hires. We start by calculating our test statistic (the difference in means) for the observed data, then simulate from the null distribution (where the labels can be interchanged) and calculate our p-value.\n\npermTest(Salary ~ Sex, data = case0102, alternative = \"less\")\n\n\n    ** Permutation test **\n\n Permutation test with alternative: less \n Observed statistic\n  Female :  5139     Male :  5957 \n Observed difference: -818 \n\n Mean of permutation distribution: -1.15 \n Standard error of permutation distribution: 156 \n P-value:  1e-04 \n\n    *-------------*\n\n\n\n\n\n\n\n\n\nThrough the permutation test, we observe that the mean starting salary for males is significantly larger than the mean starting salary for females, as we never see a permuted difference in means close to our observed value. Therefore, we reject the null hypothesis (p &lt; 0.001) and conclude that the salaries of the men are higher than that of the women back in the population",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Drawing Statistical Conclusions</span>"
    ]
  },
  {
    "objectID": "chapter02.html",
    "href": "chapter02.html",
    "title": "2  Inference Using t-Distributions",
    "section": "",
    "text": "3 Evidence Supporting Darwin’s Theory of Natural Selection\nIn this chapter we need to load the following packages\nDo birds evolve to adapt to their environments? That’s the question being addressed by Case Study 2.1 in the Sleuth.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference Using t-Distributions</span>"
    ]
  },
  {
    "objectID": "chapter02.html#statistical-summary-and-graphical-display",
    "href": "chapter02.html#statistical-summary-and-graphical-display",
    "title": "2  Inference Using t-Distributions",
    "section": "3.1 Statistical summary and graphical display",
    "text": "3.1 Statistical summary and graphical display\n\nsummary(case0201)\n\n      Year          Depth       \n Min.   :1976   Min.   : 6.200  \n 1st Qu.:1976   1st Qu.: 9.100  \n Median :1977   Median : 9.900  \n Mean   :1977   Mean   : 9.804  \n 3rd Qu.:1978   3rd Qu.:10.500  \n Max.   :1978   Max.   :11.700  \n\n\n\ncase0201 %&gt;%\n  group_by(Year) %&gt;%\n  summarize(min = min(Depth), Q1 = quantile(Depth, probs = .25),\n            median = median(Depth), Q3 = quantile(Depth, probs = .75),\n            max = max(Depth), mean = mean(Depth), sd = sd(Depth))\n\n# A tibble: 2 × 8\n   Year   min    Q1 median    Q3   max  mean    sd\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1976   6.2   8.9    9.7  10.2  11.7  9.47 1.04 \n2  1978   7.1   9.6   10.3  10.7  11.7 10.1  0.906\n\n\nA total of 178 subjects are included in the data: 89 are finches that were caught in 1976 and 89 are finches that were caught in 1978. The following are alternatives to Display 2.1 on page 30.\n\ngf_boxplot(Depth ~ factor(Year), data = case0201) + \n  xlab(\"Year\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\ngf_histogram(~Depth, data = case0201, bins = 15) +\n  facet_wrap(~Year)\n\n\n\n\n\n\n\n\n\ngf_density(~Depth, fill= ~factor(Year), data = case0201) +\n  scale_fill_brewer(\"Year\", palette = \"Set2\")\n\n\n\n\n\n\n\n\nThe distributions are approximately normally distributed, with some evidence for a long left tail.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference Using t-Distributions</span>"
    ]
  },
  {
    "objectID": "chapter02.html#inferential-procedures-two-sample-t-test",
    "href": "chapter02.html#inferential-procedures-two-sample-t-test",
    "title": "2  Inference Using t-Distributions",
    "section": "3.2 Inferential procedures (two-sample t-test)",
    "text": "3.2 Inferential procedures (two-sample t-test)\nWe can get the results of “Summary of Statistical Findings” (page 29) by using the following code:\n\n t.test(Depth ~ Year, var.equal = TRUE, data = case0201)\n\n\n    Two Sample t-test\n\ndata:  Depth by Year\nt = -4.5833, df = 176, p-value = 8.65e-06\nalternative hypothesis: true difference in means between group 1976 and group 1978 is not equal to 0\n95 percent confidence interval:\n -0.9564088 -0.3806698\nsample estimates:\nmean in group 1976 mean in group 1978 \n          9.469663          10.138202 \n\n\nNote: The assumption that the variances are equal is a relatively strong one, so if there is any doubt, assume unequal variances.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference Using t-Distributions</span>"
    ]
  },
  {
    "objectID": "chapter02.html#statistical-summary-and-graphical-display-1",
    "href": "chapter02.html#statistical-summary-and-graphical-display-1",
    "title": "2  Inference Using t-Distributions",
    "section": "4.1 Statistical summary and graphical display",
    "text": "4.1 Statistical summary and graphical display\nWe begin by reading the data and summarizing the variables.\n\nsummary(case0202)\n\n   Unaffected       Affected   \n Min.   :1.250   Min.   :1.02  \n 1st Qu.:1.600   1st Qu.:1.31  \n Median :1.770   Median :1.59  \n Mean   :1.759   Mean   :1.56  \n 3rd Qu.:1.935   3rd Qu.:1.78  \n Max.   :2.080   Max.   :2.02  \n\n\nA total of 15 subjects are included in the data. There are 15 pairs of twins; one of the twins has schizophrenia, and the other does not. So there are 15 affected subjects and 15 unaffected subjects. The difference in area of left hippocampus of these pairs of twins is:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference Using t-Distributions</span>"
    ]
  },
  {
    "objectID": "chapter02.html#inferential-procedures-paired-t-test",
    "href": "chapter02.html#inferential-procedures-paired-t-test",
    "title": "2  Inference Using t-Distributions",
    "section": "4.2 Inferential procedures (paired t-test)",
    "text": "4.2 Inferential procedures (paired t-test)\nWe can get the results displayed on page 32 by conducting a paired t-test:\n\nt.test(case0202$Unaffected, case0202$Affected, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  case0202$Unaffected and case0202$Affected\nt = 3.2289, df = 14, p-value = 0.006062\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.0667041 0.3306292\nsample estimates:\nmean difference \n      0.1986667",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference Using t-Distributions</span>"
    ]
  },
  {
    "objectID": "chapter07.html",
    "href": "chapter07.html",
    "title": "3  Simple Linear Regression: A Model for the Mean",
    "section": "",
    "text": "4 The Big Bang\nIn this chapter we need to load the following packages (remember, you will need to install packages you have never used before if you are using your own computer).\nWe will also set some options to improve legibility of graphs and output.\nIs there relation between distance and radial velocity among extra-galactic nebulae? This is the question addressed in case study 7.1 in the Sleuth.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simple Linear Regression: A Model for the Mean</span>"
    ]
  },
  {
    "objectID": "chapter07.html#summary-statistics-and-graphical-display",
    "href": "chapter07.html#summary-statistics-and-graphical-display",
    "title": "3  Simple Linear Regression: A Model for the Mean",
    "section": "4.1 Summary statistics and graphical display",
    "text": "4.1 Summary statistics and graphical display\nWe begin by reading the data (which is done when you loaded the Sleuth3 package) and summarizing the variables.\n\nsummary(case0701)\n\n    Velocity       Distance    \n Min.   :-220   Min.   :0.030  \n 1st Qu.: 165   1st Qu.:0.407  \n Median : 295   Median :0.900  \n Mean   : 373   Mean   :0.911  \n 3rd Qu.: 538   3rd Qu.:1.175  \n Max.   :1090   Max.   :2.000  \n\n\nA total of 24 nebulae are included in this data set.\n\nhist1 &lt;- gf_histogram(~Velocity, data = case0701, bins = 10)\nhist2 &lt;- gf_histogram(~Distance, data = case0701, bins = 10)\ngrid.arrange(hist1, hist2, ncol = 2)\n\n\n\n\n\n\n\n\nThe histograms are somewhat hard to interpret with the small sample sizes. I am hesitant to call the distribution of velocity multimodal. In such situations, density plots can help clarify the shape of the distribution, since their interpretation does not rely on the number of bins.\n\nd1 &lt;- gf_density(~Velocity, data = case0701, bins = 10)\nd2 &lt;- gf_density(~Distance, data = case0701, bins = 10)\ngrid.arrange(d1, d2, ncol = 2)\n\n\n\n\n\n\n\n\nThe density plots show that the distributions for the two variables are fairly symmetric, but more uniform than normally distributed.\n\ngf_point(Distance ~ Velocity, data= case0701) +\n  xlab(\"Recession Velocity (km/sec)\") +\n  ylab(\"Distance (megaparsecs)\")\n\n\n\n\n\n\n\n\nThe scatterplot is displayed on page 177 of the Sleuth. It indicates that there is a fairly strong, linear relationship between distance and velocity.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simple Linear Regression: A Model for the Mean</span>"
    ]
  },
  {
    "objectID": "chapter07.html#the-simple-linear-regression-model",
    "href": "chapter07.html#the-simple-linear-regression-model",
    "title": "3  Simple Linear Regression: A Model for the Mean",
    "section": "4.2 The simple linear regression model",
    "text": "4.2 The simple linear regression model\nThe following code presents the results interpreted on page 186 of the Sleuth.\n\nmod1 &lt;- lm(Distance ~ Velocity, data = case0701)\nsummary(mod1)\n\n\nCall:\nlm(formula = Distance ~ Velocity, data = case0701)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.7672 -0.2352 -0.0108  0.2108  0.9146 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.399170   0.118666    3.36   0.0028 ** \nVelocity    0.001372   0.000228    6.02  4.6e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.406 on 22 degrees of freedom\nMultiple R-squared:  0.623, Adjusted R-squared:  0.605 \nF-statistic: 36.3 on 1 and 22 DF,  p-value: 4.61e-06\n\n\nThe estimated parameter for the intercept is 0.3992 megaparsecs and the estimated parameter for velocity is 0.0014 megaparsecs/(km/sec). The estimated mean function is \\(\\hat{mu} (\\text{distance}|\\text{velocity}) = 0.3992 + 0.0014 (\\text{velocity})\\). The estimate of the residual standard error is 0.4056 megaparsecs with 22 degrees of freedom. These results are also presented by Display 7.9 (page 187).\nDisplay 7.8 (page 186) shows the list of fitted values and residuals for this model. We can easily obtain these using the augment function in the broom package (these are in the .fitted column).\n\nbang_augmented &lt;- augment(mod1)\nbang_augmented\n\n# A tibble: 24 × 8\n   Distance Velocity .fitted  .resid   .hat .sigma   .cooksd .std.resid\n      &lt;dbl&gt;    &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1     0.03      170  0.632  -0.602  0.0547  0.393 0.0675       -1.53  \n 2     0.03      290  0.797  -0.767  0.0438  0.378 0.0858       -1.93  \n 3     0.21     -130  0.221  -0.0108 0.122   0.415 0.0000554    -0.0283\n 4     0.26      -70  0.303  -0.0431 0.104   0.415 0.000728     -0.112 \n 5     0.28     -185  0.145   0.135  0.140   0.414 0.0104        0.358 \n 6     0.28     -220  0.0972  0.183  0.153   0.413 0.0216        0.489 \n 7     0.45      200  0.674  -0.224  0.0511  0.412 0.00863      -0.566 \n 8     0.5       290  0.797  -0.297  0.0438  0.410 0.0129       -0.749 \n 9     0.5       270  0.770  -0.270  0.0450  0.411 0.0109       -0.680 \n10     0.63      200  0.674  -0.0437 0.0511  0.415 0.000329     -0.110 \n# ℹ 14 more rows\n\n\nAlternatively, you can extract the fitted values using the fitted function and the residuals using the resid function:\n\nfitted(mod1)\n\n      1       2       3       4       5       6       7       8       9      10      11 \n0.63248 0.79717 0.22076 0.30310 0.14528 0.09724 0.67365 0.79717 0.76972 0.67365 0.81089 \n     12      13      14      15      16      17      18      19      20      21      22 \n0.35800 1.29124 0.60503 1.08537 1.66179 1.01675 1.08537 1.08537 1.71668 1.08537 1.56572 \n     23      24 \n1.49710 1.89509 \n\nresid(mod1)\n\n       1        2        3        4        5        6        7        8        9       10 \n-0.60248 -0.76717 -0.01076 -0.04310  0.13472  0.18276 -0.22365 -0.29717 -0.26972 -0.04365 \n      11       12       13       14       15       16       17       18       19       20 \n-0.01089  0.54200 -0.39124  0.29497 -0.18537 -0.66179  0.08325  0.01463  0.31463 -0.01668 \n      21       22       23       24 \n 0.91463  0.43428  0.50290  0.10491 \n\n\nThe sum of the squared residuals is 3.62 and R-squared is 0.6062.\n\nsum(resid(mod1)^2)\n\n[1] 3.62\n\nsum(resid(mod1)^2) / sum((fitted(mod1) - mean(case0701$Distance))^2)\n\n[1] 0.6062\n\n\nWe can also display 95% confidence bands for the model line and the predicted values, the following graph is akin to Display 7.11 (page 191).\n\ngf_point(Distance ~ Velocity, data = case0701) %&gt;%\n  gf_lm(interval = \"prediction\") %&gt;%\n  gf_lm(interval = \"confidence\", alpha = 0.5) \n\nWarning: Using the `size` aesthetic with geom_ribbon was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\nWarning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simple Linear Regression: A Model for the Mean</span>"
    ]
  },
  {
    "objectID": "chapter07.html#inferential-tools",
    "href": "chapter07.html#inferential-tools",
    "title": "3  Simple Linear Regression: A Model for the Mean",
    "section": "4.3 Inferential Tools",
    "text": "4.3 Inferential Tools\nFirst, we test \\(\\beta_0\\) (the intercept). From the previous summary, we know that the two-sided p-value for the intercept is 0.0028. This p-value is small enough for us to reject the null hypothesis that the estimated parameter for the intercept equals 0 (page 188).\nNext, we want to examine \\(\\beta_1\\). The current \\(\\beta_1\\) for \\(\\hat{\\mu}(Y|X) = \\beta_0 + \\beta_1 X\\) is 0.0014, and we want to get the \\(\\beta_1\\) for \\(\\hat{\\mu}(Y|X) = \\beta_1 X\\), a model with no intercept (page 188)\n\n# linear regression with no intercept\nmod2 &lt;- lm(Distance ~ Velocity - 1, data = case0701)\nsummary(mod2)\n\n\nCall:\nlm(formula = Distance ~ Velocity - 1, data = case0701)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.7677 -0.0691  0.2295  0.4606  1.0393 \n\nCoefficients:\n         Estimate Std. Error t value Pr(&gt;|t|)    \nVelocity 0.001921   0.000191      10    7e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.488 on 23 degrees of freedom\nMultiple R-squared:  0.814, Adjusted R-squared:  0.806 \nF-statistic:  101 on 1 and 23 DF,  p-value: 7.05e-10\n\nconfint(mod2)\n\n            2.5 %   97.5 %\nVelocity 0.001526 0.002317\n\n\nWithout the intercept, the new estimate for \\(\\beta_1\\) is 0.0019 megaparsec-second/km. The standard error is \\(1.91 \\times 10^{-4}\\) megaparsecs with 23 degrees of freedom. The 95% confidence interval is (0.0015, 0.0023). Because 1 megaparsec-second/km = 979.8 billion years, the confidence interval could be written as 1.49 to 2.27 billion years, and the best estimate is 1.88 billion years (page 188).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simple Linear Regression: A Model for the Mean</span>"
    ]
  },
  {
    "objectID": "chapter07.html#summary-statistics-and-graphical-display-1",
    "href": "chapter07.html#summary-statistics-and-graphical-display-1",
    "title": "3  Simple Linear Regression: A Model for the Mean",
    "section": "5.1 Summary statistics and graphical display",
    "text": "5.1 Summary statistics and graphical display\nWe begin by loading the data and summarizing the variables.\n\nsummary(case0702)\n\n      Time           pH      \n Min.   :1.0   Min.   :5.36  \n 1st Qu.:2.0   1st Qu.:5.64  \n Median :4.0   Median :6.03  \n Mean   :4.2   Mean   :6.12  \n 3rd Qu.:6.0   3rd Qu.:6.49  \n Max.   :8.0   Max.   :7.02  \n\n\nA total of 10 steer carcasses are included in this data as shown in Display 7.3, page 179.\n\ngf_point(pH ~ log(Time), data = case0702)\n\n\n\n\n\n\n\n\nThe above scatterplot indicates a negative linear relationship between pH and log(Time).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simple Linear Regression: A Model for the Mean</span>"
    ]
  },
  {
    "objectID": "chapter07.html#the-simple-linear-regression-model-1",
    "href": "chapter07.html#the-simple-linear-regression-model-1",
    "title": "3  Simple Linear Regression: A Model for the Mean",
    "section": "5.2 The simple linear regression model",
    "text": "5.2 The simple linear regression model\nWe fit a simple linear regression model of pH on log(time) after slaughter. The estimated mean function will be \\(\\hat{\\mu} = (\\text{pH}|\\text{logtime}) = \\beta_0 + \\beta_1  \\log(\\text{Time})\\).\n\nmod3 &lt;- lm(pH ~ log(Time), data = case0702)\nsummary(mod3)\n\n\nCall:\nlm(formula = pH ~ log(Time), data = case0702)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.1147 -0.0589  0.0209  0.0361  0.1166 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6.9836     0.0485   143.9  6.1e-15 ***\nlog(Time)    -0.7257     0.0344   -21.1  2.7e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0823 on 8 degrees of freedom\nMultiple R-squared:  0.982, Adjusted R-squared:  0.98 \nF-statistic:  444 on 1 and 8 DF,  p-value: 2.7e-08\n\ncoef(mod3)\n\n(Intercept)   log(Time) \n     6.9836     -0.7257 \n\n\n\\(\\hat{\\beta}_0 = 6.9836\\), \\(\\hat{\\beta}_1 = -0.7257\\), and \\(\\hat{\\sigma} = 0.0823\\). (See page 189.)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simple Linear Regression: A Model for the Mean</span>"
    ]
  },
  {
    "objectID": "chapter07.html#inferential-tools-1",
    "href": "chapter07.html#inferential-tools-1",
    "title": "3  Simple Linear Regression: A Model for the Mean",
    "section": "5.3 Inferential Tools",
    "text": "5.3 Inferential Tools\nWith the previous information, we can calculate the 95% confidence interval for the estimated mean pH of steers 4 hours after slaughter (Display 7.10, page 189):\n\nbeta0 &lt;- coef(mod3)[1]\nbeta1 &lt;- coef(mod3)[2]\nmu &lt;- beta0 + beta1 * log(4)\n\nn &lt;- nrow(case0702)\nsigma_hat &lt;- sigma(mod3)\nmeanx &lt;- mean(log(case0702$Time))\nsdx &lt;- sd(log(case0702$Time))\n\nse &lt;- sigma_hat * sqrt(1/n + (log(4) - meanx)^2/((n - 1) * sdx))\nse\n\n[1] 0.0267\n\nupper &lt;- mu + qt(0.975, df = 8) * se\nupper\n\n(Intercept) \n      6.039 \n\nlower &lt;- mu - qt(0.975, df = 8) * se\nlower\n\n(Intercept) \n      5.916 \n\n\nOr we can use the following code to get the same result:\n\npredict(mod3, newdata = data.frame(Time = 4), interval = \"confidence\")\n\n    fit   lwr  upr\n1 5.978 5.916 6.04\n\n\nSo the 95% confidence interval for estimated mean is (5.92, 6.04).\nNext, we can calculate the 95% prediction interval for a steer carcass 4 hours after slaughter (Display 7.12, page 193):\n\npred &lt;- beta0 + beta1 * log(4)\npred\n\n(Intercept) \n      5.978 \n\npred_se &lt;- sigma_hat * sqrt(1 + 1/n + (log(4) - meanx)^2/((n - 1) * sdx))\npred_se\n\n[1] 0.08648\n\npred_upper &lt;- pred + qt(0.975, df = 8) * pred_se\npred_upper\n\n(Intercept) \n      6.177 \n\npred_lower &lt;- pred - qt(0.975, df = 8) * pred_se\npred_lower\n\n(Intercept) \n      5.778 \n\n\nOr we can use the following code to get the 95% prediction interval for a steer carcass 4 hours after slaughter:\n\npredict(mod3, newdata = data.frame(Time = 4), interval = \"prediction\")\n\n    fit   lwr   upr\n1 5.978 5.778 6.177\n\n\nSo the 95% prediction interval is (5.78, 6.18).\nThe 95% prediction band is presented as Display 7.4 (page 180):\n\ngf_point(pH ~ log(Time), data = case0702) %&gt;%\n  gf_lm(interval = \"prediction\") %&gt;%\n  gf_lm(interval = \"confidence\", alpha = 0.5)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simple Linear Regression: A Model for the Mean</span>"
    ]
  },
  {
    "objectID": "chapter08.html",
    "href": "chapter08.html",
    "title": "4  A Closer Look at the Assumptions for Simple Linear Regression",
    "section": "",
    "text": "5 Introduction\nThis document is intended to help describe how to undertake analyses introduced as examples in the Third Edition of the Statistical Sleuth (2013) by Fred Ramsey and Dan Schafer. More information about the book can be found at http://www.proaxis.com/~panorama/home.htm.\nThis work adapts work done by Linda Loi, Ruobing Zhang, Kate Aloisio, and Nicholas J. Horton. Their work leveraged initiatives undertaken by Project MOSAIC (http://www.mosaic-web.org), an NSF-funded effort to improve the teaching of statistics, calculus, science and computing in the undergraduate curriculum.\nIn this chapter we need to load the following packages (remember, you will need to install packages you have never used before if you are using your own computer).\nlibrary(ggformula) # graphics\nlibrary(Sleuth3)   # Sleuth data sets\nlibrary(broom)     # extract pieces of lm output\nlibrary(gridExtra) # arrange multiple plots on a page\nWe will also set some options to improve legibility of graphs and output.\noptions(digits=4) # display four significant digits by default",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Closer Look at the Assumptions for Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapter08.html#summary-statistics-and-graphical-display",
    "href": "chapter08.html#summary-statistics-and-graphical-display",
    "title": "4  A Closer Look at the Assumptions for Simple Linear Regression",
    "section": "6.1 Summary statistics and graphical display",
    "text": "6.1 Summary statistics and graphical display\nWe begin by reading the data and summarizing the variables.\n\nsummary(case0801)\n\n      Area            Species     \n Min.   :    1.0   Min.   :  7.0  \n 1st Qu.:   18.5   1st Qu.: 13.5  \n Median : 3435.0   Median : 45.0  \n Mean   :11615.1   Mean   : 48.6  \n 3rd Qu.:16807.5   3rd Qu.: 76.5  \n Max.   :44218.0   Max.   :108.0  \n\n\nA total of 7 islands are included in this data as displayed in Display 8.1 (page 208). We can then observe the relationship between the area and the number of species for these islands with a scatterplot, akin to the top figure in Display 8.2 (page 209).\n\ngf_point(Species ~ Area, data = case0801) %&gt;%\n  gf_labs(x = \"Area of Island (square miles)\")\n\n\n\n\n\n\n\n\nIt appears that the relationship with the observed values is not linear, one way to address this issue is to transform the variables. Below is a scatterplot where both variables have been log transformed.\n\ngf_point(log(Species) ~ log(Area), data = case0801) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(x = \"Area of Island (square miles) (log scale)\",\n          y = \"Species (log scale)\")\n\nWarning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Closer Look at the Assumptions for Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapter08.html#simple-linear-model",
    "href": "chapter08.html#simple-linear-model",
    "title": "4  A Closer Look at the Assumptions for Simple Linear Regression",
    "section": "6.2 Simple Linear Model",
    "text": "6.2 Simple Linear Model\nWe first fit the model for \\(\\mu\\{\\log(Species)|\\log(Area)\\} = \\beta_0 + \\beta_1 \\cdot\\log(Area)\\). This can be done by applying the log transformation directly in the lm() statement.\n\nlm1 &lt;- lm(log(Species) ~ log(Area), data = case0801)\nsummary(lm1)\n\n\nCall:\nlm(formula = log(Species) ~ log(Area), data = case0801)\n\nResiduals:\n        1         2         3         4         5         6         7 \n-0.002136  0.176975 -0.215487  0.000947 -0.029244  0.059543  0.009402 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.9365     0.0881    22.0  3.6e-06 ***\nlog(Area)     0.2497     0.0121    20.6  5.0e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.128 on 5 degrees of freedom\nMultiple R-squared:  0.988, Adjusted R-squared:  0.986 \nF-statistic:  425 on 1 and 5 DF,  p-value: 4.96e-06\n\n\nNote: an alternative approach is to create transformed variables in the data set. This is easily done using the mutate() command in the dplyr package:\n\ncase0801 &lt;- mutate(case0801, logspecies = log(Species), logarea = log(Area))\nlm1 &lt;- lm(logspecies ~ logarea, data = case0801)\n\nThus our estimated equation becomes, \\(\\widehat{\\mu} \\{\\log(Species)|\\log(Area)\\} = 1.94 + 0.25 \\log(Area)\\). Next, we calculate the 95% confidence interval for the estimates, note that the logarea 95% confidence interval is interpreted in the “Statistical Conclusion” on page 208:\n\nconfint(lm1)\n\n             2.5 % 97.5 %\n(Intercept) 1.7100 2.1631\nlogarea     0.2186 0.2808\n\n\nTo interpret this log-log model the Sleuth notes that if \\(\\widehat{\\mu} \\{\\log(Species)|\\log(Area)\\} = \\beta_0 + \\beta_1 * \\log(X)\\) then \\(\\text{median}\\{Y|X \\} = \\exp(\\beta_0)X^{beta_1}\\) (page 217). For this example the researchers are interested in a doubling effect (\\(2\\beta_1\\)). Therefore to obtain the 95% confidence interval for the multiplicative factor in the median we used the following code:\n\n2^confint(lm1)\n\n            2.5 % 97.5 %\n(Intercept) 3.272  4.479\nlogarea     1.164  1.215\n\n\nThus for this model the estimated median number of species is 1.19 (\\(2^{0.25}\\)) with a 95% confidence interval between (1.16, 1.21). These match the numbers found on page 217.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Closer Look at the Assumptions for Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapter08.html#assessment-of-assumptions",
    "href": "chapter08.html#assessment-of-assumptions",
    "title": "4  A Closer Look at the Assumptions for Simple Linear Regression",
    "section": "6.3 Assessment of Assumptions",
    "text": "6.3 Assessment of Assumptions\nFirst we will have to assume independence from the information given. As seen in the above Q-Q plots, the observations for each variable were not normally distributed, once we performed a log transformation the distribution of the values became more approximately normal. While it can be useful to check the distribution of the response variable prior to modeling, it’s still important to check the distribution of the residuals.\nNext, we can check for linearity of the mean function, \\(\\widehat{\\mu} \\{\\log(Species)|\\log(Area)\\}\\) using either a plot of the residuals vs. the fitted values or a plot of the residuals vs. \\(\\log(Area)\\) (these will provide the same information). Using the ggformula toolkit, we first create an augmented data set where we add the residuals and fitted values.\n\naug1 &lt;- augment(lm1)\nres1 &lt;- gf_point(.resid ~ .fitted, data = aug1) %&gt;%\n  gf_hline(yintercept = 0, col = \"blue\", lty = 2) %&gt;%\n  gf_labs(x = \"Fitted values\", y = \"Residuals\",  title = \"Residuals vs. fitted values\")\n\nres2 &lt;- gf_point(.resid ~ logarea, data = aug1) %&gt;%\n  gf_hline(yintercept = 0, col = \"blue\", lty = 2) %&gt;%\n  gf_labs(x = \"Fitted values\", y = \"Residuals\",  title = \"Residuals vs. fitted values\")\n\ngrid.arrange(res1, res2, ncol = 2)\n\n\n\n\n\n\n\n\nThe residual plot shows no apparent curavture, so the assumption of linearity appears to be valid. Additionally, there are no signs of non-constant variance (don’t over-interpret the two larger residuals here, with so few points this is more likely due to sampling variability).\nTo finish, we examine the distribution of the standardized residuals. The Q-Q plot exhibits no substantial deviations from the assumption of normality.\n\ngf_qq(~.std.resid, data = aug1) %&gt;%\n  gf_qqline() %&gt;%\n  gf_labs(x = \"N(0, 1) quantiles\", y = \"Standardized residuals\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Closer Look at the Assumptions for Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapter08.html#summary-statistics-and-graphical-display-1",
    "href": "chapter08.html#summary-statistics-and-graphical-display-1",
    "title": "4  A Closer Look at the Assumptions for Simple Linear Regression",
    "section": "7.1 Summary statistics and graphical display",
    "text": "7.1 Summary statistics and graphical display\nWe begin by reading the data and summarizing the variables.\n\n summary(case0802)\n\n      Time            Voltage        Group   \n Min.   :   0.09   Min.   :26.0   Group1: 3  \n 1st Qu.:   1.62   1st Qu.:31.5   Group2: 5  \n Median :   6.92   Median :34.0   Group3:11  \n Mean   :  98.56   Mean   :33.1   Group4:15  \n 3rd Qu.:  38.38   3rd Qu.:36.0   Group5:19  \n Max.   :2323.70   Max.   :38.0   Group6:15  \n                                  Group7: 8  \n\n\nA total of 76 samples of insulating fluids are included in this data. Each sample was placed in one of 7 groups representing different degrees of voltage. Each group varied in sample size as shown in Display 8.4 (page 211). Below is a version of Display 8.4 rendered in R (and without the dual y-axis).\n\ngf_point(log(Time) ~ Voltage, data = case0802) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(x = \"Voltage Level (kV)\", y = \"Breakdown Time (minutes) (log scale)\") %&gt;%\n  gf_refine(scale_x_continuous(breaks = seq(26, 38, by = 2)))\n\n\n\n\n\n\n\n\nTo see why a log transformation was initially proposed for breakdown time, consider the histogram of breakdown times (left).\n\nh3 &lt;- gf_histogram(~Time, data = case0802, bins = 10) \nh4 &lt;- gf_histogram(~log(Time), data = case0802, bins = 10)\ngrid.arrange(h3, h4, ncol = 2)\n\n\n\n\n\n\n\n\nThe distribution of Time is highly skewed with a long right tail. In such cases, the natural log often helps “normalize” the variables (as seen on the right).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Closer Look at the Assumptions for Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapter08.html#simple-linear-regression-models",
    "href": "chapter08.html#simple-linear-regression-models",
    "title": "4  A Closer Look at the Assumptions for Simple Linear Regression",
    "section": "7.2 Simple linear regression models",
    "text": "7.2 Simple linear regression models\nThe model that the researchers want to analyze is \\(\\mu \\{\\log(Time)|Voltage\\} = \\beta_0 + \\beta_1 \\cdot Voltage\\).\n\nlm2 &lt;- lm(log(Time) ~ Voltage, data = case0802)\nsummary(lm2)\n\n\nCall:\nlm(formula = log(Time) ~ Voltage, data = case0802)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.029 -0.692  0.037  1.209  2.651 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  18.9555     1.9100    9.92  3.1e-15 ***\nVoltage      -0.5074     0.0574   -8.84  3.3e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.56 on 74 degrees of freedom\nMultiple R-squared:  0.514, Adjusted R-squared:  0.507 \nF-statistic: 78.1 on 1 and 74 DF,  p-value: 3.34e-13\n\n\nThe estimated model is \\(\\widehat{\\mu} \\{\\log(Time)|Voltage\\} = 18.96 -0.51 \\log(Area)\\). The \\(R^2\\) for the model is 51.36%, as discussed on page 222. To interpret the model we first exponentiate the estimated coefficients because the response variable is logged, as shown on page 216.\n\nexp(coef(lm2))\n\n(Intercept)     Voltage \n  1.707e+08   6.021e-01 \n\n\nA 1 kV increase in voltage is associated with a multiplicative change in the median breakdown time of 0.6.\nNext we can calculate 95% confidence intervals for the slope and intercept. (Remember that we still need to exponentiate the output in order to interpret it.)\n\nexp(confint(lm2))\n\n                2.5 %    97.5 %\n(Intercept) 3.797e+06 7.675e+09\nVoltage     5.370e-01 6.750e-01\n\n\nThe 95% confidence interval for the multiplicative change in median breakdown time is (0.54, 0.68), as seen on page 216.\nNext, we can assess the fit using the Analysis of Variance (ANOVA) as outlined in section 8.5. The ANOVA results below match those in the top half of Display 8.8 (page 219).\n\nanova(lm2)\n\nAnalysis of Variance Table\n\nResponse: log(Time)\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nVoltage    1    190   190.2    78.1 3.3e-13 ***\nResiduals 74    180     2.4                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can then compare this with a model with separate means for each group.\n\nlm3 &lt;- lm(log(Time) ~ as.factor(Voltage), data = case0802)\nsummary(lm3)\n\n\nCall:\nlm(formula = log(Time) ~ as.factor(Voltage), data = case0802)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.868 -0.819  0.074  1.122  3.143 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             5.624      0.916    6.14  4.7e-08 ***\nas.factor(Voltage)28   -0.294      1.159   -0.25  0.80019    \nas.factor(Voltage)30   -1.802      1.034   -1.74  0.08571 .  \nas.factor(Voltage)32   -3.395      1.004   -3.38  0.00118 ** \nas.factor(Voltage)34   -3.838      0.986   -3.89  0.00023 ***\nas.factor(Voltage)36   -4.722      1.004   -4.70  1.3e-05 ***\nas.factor(Voltage)38   -6.048      1.074   -5.63  3.6e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.59 on 69 degrees of freedom\nMultiple R-squared:  0.531, Adjusted R-squared:  0.49 \nF-statistic:   13 on 6 and 69 DF,  p-value: 8.87e-10\n\n\nThis model has a F-statistic of 13 with a p-value &lt; 0.0001, as shown in the bottom half of Display 8.8 (page 218).\n\nanova(lm3)\n\nAnalysis of Variance Table\n\nResponse: log(Time)\n                   Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nas.factor(Voltage)  6    196    32.7      13 8.9e-10 ***\nResiduals          69    174     2.5                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote that the values for the Residuals can also be found in the bottom half of Display 8.8 (page 219).\nThe F-statistic and its associated p-value for the lack-of-fit discussion on page 220 can be calculated by comparing the two models with an ANOVA.\n\nanova(lm2, lm3)\n\nAnalysis of Variance Table\n\nModel 1: log(Time) ~ Voltage\nModel 2: log(Time) ~ as.factor(Voltage)\n  Res.Df RSS Df Sum of Sq   F Pr(&gt;F)\n1     74 180                        \n2     69 174  5      6.33 0.5   0.77",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Closer Look at the Assumptions for Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapter08.html#assessment-of-assumptions-1",
    "href": "chapter08.html#assessment-of-assumptions-1",
    "title": "4  A Closer Look at the Assumptions for Simple Linear Regression",
    "section": "7.3 Assessment of Assumptions",
    "text": "7.3 Assessment of Assumptions\nFirst we will have to assume independence for the information given.\nNext, we can check for linearity. The following figure is akin to the right panel in Display 8.14 (page 226).\n\naug2 &lt;- augment(lm2)\n\ngf_qq(~.std.resid, data = aug2) %&gt;%\n  gf_qqline() %&gt;%\n  gf_labs(x = \"Normal quantiles\", y = \"Standarized residuals\")\n\n\n\n\n\n\n\n\nThe points on the Q-Q plot are close enough to the straight line that we can assume the assumption of normally distribution residuals is reasonable.\nTo assess linearity and constant error variance (i.e. homoscedasticity) we turn to a residual plot.\n\ngf_point(.resid ~ .fitted, data = aug2) %&gt;%\n  gf_hline(yintercept = 0, linetype = 2, color = \"blue\") %&gt;%\n  gf_labs(x = \"Fitted values\", y = \"Residuals\")\n\n\n\n\n\n\n\n\nThere is no apparent curvature seen on the residual plot; however, the variability does not appear to be constant across all of the fitted values (or Voltage if you plot the residuals vs. X).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Closer Look at the Assumptions for Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapter09.html",
    "href": "chapter09.html",
    "title": "5  Multiple Regression",
    "section": "",
    "text": "6 Effects of light on meadowfoam flowering\nIn this chapter we need to load the following packages (remember, you will need to install packages you have never used before if you are using your own computer).\nWe will also set some options to improve legibility of graphs and output.\nDo different amounts of light affect the growth of meadowfoam (a small plant used to create seed oil)? This is the question addressed in case study 9.1 in the Sleuth.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapter09.html#data-coding-summary-statistics-and-graphical-display",
    "href": "chapter09.html#data-coding-summary-statistics-and-graphical-display",
    "title": "5  Multiple Regression",
    "section": "6.1 Data coding, summary statistics and graphical display",
    "text": "6.1 Data coding, summary statistics and graphical display\nWe begin by reading the data.\n\nhead(case0901)\n\n  Flowers Time Intensity\n1    62.3    1       150\n2    77.4    1       150\n3    55.3    1       300\n4    54.2    1       300\n5    49.6    1       450\n6    61.9    1       450\n\n\nThe original data set codes the Time variable as 1 = Late and 2 = Early. The code chunk below changes Time to a factor with informative labels (rather than using the original numeric labels) and obtains overall summaries of each variable.\n\ncase0901 &lt;- mutate(case0901, Time = factor(Time, labels = c(\"Late\", \"Early\")))\nsummary(case0901)\n\n    Flowers        Time      Intensity  \n Min.   :31.3   Late :12   Min.   :150  \n 1st Qu.:45.4   Early:12   1st Qu.:300  \n Median :54.8              Median :525  \n Mean   :56.1              Mean   :525  \n 3rd Qu.:64.5              3rd Qu.:750  \n Max.   :78.0              Max.   :900  \n\n\nA total of 24 meadowfoam plants were included in this data. There were 12 treatment groups: 6 light intensities at each of the 2 timing levels (Display 9.2, page 239 of the Sleuth). The following code generates the scatterplot of the average number of flowers per plant versus the applied light intensity for each of the 12 experimental units akin to Display 9.3 on page 240.\n\ngf_point(Flowers ~ Intensity, data = case0901, color = ~Time, shape = ~Time) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(x = expression(paste(\"Light intensity (\", mu, \"mol/\", m^{2},\"/sec)\")),\n          y = \"Flowers per plant\")\n\nWarning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapter09.html#multiple-linear-regression-model",
    "href": "chapter09.html#multiple-linear-regression-model",
    "title": "5  Multiple Regression",
    "section": "6.2 Multiple linear regression model",
    "text": "6.2 Multiple linear regression model\nWe next fit a multiple linear regression model that specifies parallel regression lines for the mean number of flowers as a function of light intensity as interpreted on page 239.\n\nlm1 &lt;- lm(Flowers ~ Intensity + Time, data = case0901)\nsummary(lm1)\n\n\nCall:\nlm(formula = Flowers ~ Intensity + Time, data = case0901)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -9.65  -4.14  -1.56   5.63  12.16 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 71.30583    3.27377   21.78  6.8e-16 ***\nIntensity   -0.04047    0.00513   -7.89  1.0e-07 ***\nTimeEarly   12.15833    2.62956    4.62  0.00015 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.44 on 21 degrees of freedom\nMultiple R-squared:  0.799, Adjusted R-squared:  0.78 \nF-statistic: 41.8 on 2 and 21 DF,  p-value: 4.79e-08\n\n\nWe can also fit a multiple linear regression with an interaction between light intensity and timing of its initiation as shown in Display 9.14 (page 260) and interpreted on page 239. Notice that in the regression formula multiplication, i.e. Intensity * Time, specifies that both variables and their interaction should be used as explanatory variables in the model.\n\nlm2 &lt;- lm(Flowers ~ Intensity * Time, data = case0901)\nsummary(lm2)\n\n\nCall:\nlm(formula = Flowers ~ Intensity * Time, data = case0901)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -9.52  -4.28  -1.42   5.47  11.94 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         71.62333    4.34330   16.49  4.1e-13 ***\nIntensity           -0.04108    0.00744   -5.52  2.1e-05 ***\nTimeEarly           11.52333    6.14236    1.88    0.075 .  \nIntensity:TimeEarly  0.00121    0.01051    0.12    0.910    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.6 on 20 degrees of freedom\nMultiple R-squared:  0.799, Adjusted R-squared:  0.769 \nF-statistic: 26.5 on 3 and 20 DF,  p-value: 3.55e-07\n\n\nWe can graphically display these models as shown in Display 9.8 (page 251).\n\nequal_line &lt;- gf_point(Flowers ~ Intensity, data = case0901, color = ~Time, shape = ~Time) %&gt;%\n  gf_lm(Flowers ~ Intensity, data = case0901, inherit = FALSE) %&gt;%\n  gf_labs(x = expression(paste(\"Light intensity (\", mu, \"mol/\", m^{2},\"/sec)\")),\n          y = \"Flowers per plant\")\n\naug_lm1 &lt;- augment(lm1)\nparallel_lines &lt;- gf_point(Flowers ~ Intensity, data = case0901, color = ~Time, shape = ~Time) %&gt;%\n  gf_lm(.fitted ~ Intensity, data = aug_lm1) %&gt;%\n  gf_labs(x = expression(paste(\"Light intensity (\", mu, \"mol/\", m^{2},\"/sec)\")),\n          y = \"Flowers per plant\")\n\nseparate_lines &lt;- gf_point(Flowers ~ Intensity, data = case0901, color = ~Time, shape = ~Time) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(x = expression(paste(\"Light intensity (\", mu, \"mol/\", m^{2},\"/sec)\")),\n          y = \"Flowers per plant\")\n\ngrid.arrange(separate_lines, parallel_lines, equal_line, ncol = 1)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapter09.html#graphical-presentation",
    "href": "chapter09.html#graphical-presentation",
    "title": "5  Multiple Regression",
    "section": "7.1 Graphical presentation",
    "text": "7.1 Graphical presentation\nThe following displays a simple (unadorned) pairs plot, similar to Display 9.10 on page 255.\n\npairs(Brain ~ Body + Gestation + Litter, data = case0902)\n\n\n\n\n\n\n\n\nFancier options are available. If you install the car package, then you can use the scatterplotMatrix() command (notice the difference in the formula syntax):\n\nlibrary(car)\nscatterplotMatrix(~Brain + Body + Gestation + Litter, data = case0902)\n\n\n\n\n\n\n\n\nBy default both a smoother and a regression line are drawn by scatterplotMatrix(), to omit these add the arguments smooth = FALSE and/or regLine = FALSE. For more information about customizations, see the help menu for the function.\nFinally, you can render ggplot2-style scatterplot matrices via the ggpairs function in the GGally package.\n\nlibrary(GGally)\nggpairs(data = case0902, columns = c(\"Brain\", \"Body\", \"Gestation\", \"Litter\"))\n\n\n\n\n\n\n\n\nThe following displays a jittered scatterplot of log brain weight as a function of log litter size, similar to Display 9.12 on page 258.\n\ngf_jitter(log(Brain) ~ Litter, data = case0902) %&gt;%\n  gf_labs(x = \"Litter size (jittered)\", y = \"Brain Weight (g) (log scale)\") %&gt;%\n  gf_refine(scale_x_continuous(breaks = 1:8))\n\n\n\n\n\n\n\n\nThe following displays a jittered scatterplot using the original data stratified by body weight on a log-transformed axis, similar to Display 9.13 on page 259.\n\ncase0902 &lt;- mutate(case0902, \n                   weightcut = cut(Body, breaks = c(0, 2.1, 9.1, 100, 4200),\n                                   labels = c(\"Body Weight: 0kg to 2.1kg\", \n                                              \"Body Weight: 2.1kg to 9.1kg\", \n                                              \"Body Weight: 9.1kg to 100kg\",\n                                              \"Body Weight: 100 to 4,200\")))\n\ngf_point(log(Brain) ~ Gestation, data = case0902) %&gt;%\n  gf_facet_wrap(~weightcut, ncol = 1) %&gt;%\n  gf_labs(x = \"Gestation length (days)\", y = \"Brain Weight (g) (log scale)\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapter09.html#multiple-linear-regression-model-1",
    "href": "chapter09.html#multiple-linear-regression-model-1",
    "title": "5  Multiple Regression",
    "section": "7.2 Multiple linear regression model",
    "text": "7.2 Multiple linear regression model\nThe following model is interpreted on page 240 and shown in Display 9.15 (page 260).\n\nlm3 &lt;- lm(log(Brain) ~ log(Body) + log(Gestation) + log(Litter), data = case0902)\nsummary(lm3)\n\n\nCall:\nlm(formula = log(Brain) ~ log(Body) + log(Gestation) + log(Litter), \n    data = case0902)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9541 -0.2964 -0.0311  0.2811  1.5749 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      0.8548     0.6617    1.29   0.1996    \nlog(Body)        0.5751     0.0326   17.65   &lt;2e-16 ***\nlog(Gestation)   0.4179     0.1408    2.97   0.0038 ** \nlog(Litter)     -0.3101     0.1159   -2.67   0.0089 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.475 on 92 degrees of freedom\nMultiple R-squared:  0.954, Adjusted R-squared:  0.952 \nF-statistic:  632 on 3 and 92 DF,  p-value: &lt;2e-16",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapter10.html",
    "href": "chapter10.html",
    "title": "6  Inferential Tools for Multiple Regression",
    "section": "",
    "text": "7 Galileo’s data on the motion of falling bodies\nIn this chapter we need to load the following packages (remember, you will need to install packages you have never used before if you are using your own computer).\nWe will also set some options to improve legibility of graphs and output.\nGalileo investigated the relationship between height and horizontal distance. This is the question addressed in case study 10.1 in the Sleuth.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferential Tools for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapter10.html#data-coding-summary-statistics-and-graphical-display",
    "href": "chapter10.html#data-coding-summary-statistics-and-graphical-display",
    "title": "6  Inferential Tools for Multiple Regression",
    "section": "7.1 Data coding, summary statistics and graphical display",
    "text": "7.1 Data coding, summary statistics and graphical display\nWe begin by reading the data and summarizing the variables.\n\nsummary(case1001)\n\n    Distance       Height    \n Min.   :253   Min.   : 100  \n 1st Qu.:366   1st Qu.: 250  \n Median :451   Median : 450  \n Mean   :434   Mean   : 493  \n 3rd Qu.:514   3rd Qu.: 700  \n Max.   :573   Max.   :1000  \n\n\nThere were a total of 7 trials of Galileo’s experiment. For each trial, he recorded the initial height and then measured the horizontal distance as shown in Display 10.1 (page 272). We can start to explore this relationship by creating a scatterplot of Galileo’s horizontal distances versus initial heights. The following graph is akin to Display 10.2 (page 273).\n\ngf_point(Distance ~ Height, data = case1001) %&gt;%\n  gf_labs(x = \"Horizontal distance (punti)\", y = \"Initial height (punti)\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferential Tools for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapter10.html#models",
    "href": "chapter10.html#models",
    "title": "6  Inferential Tools for Multiple Regression",
    "section": "7.2 Models",
    "text": "7.2 Models\nThe first model created is a cubic model as interpreted on page 273 and summarized in Display 10.13 (page 291).\n\ngalileo_lm1 &lt;- lm(Distance ~ Height + I(Height^2) + I(Height^3), data = case1001)\nsummary(galileo_lm1)\n\n\nCall:\nlm(formula = Distance ~ Height + I(Height^2) + I(Height^3), data = case1001)\n\nResiduals:\n      1       2       3       4       5       6       7 \n-2.4036  3.5809  1.8917 -4.4688 -0.0804  2.3216 -0.8414 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.56e+02   8.33e+00   18.71  0.00033 ***\nHeight       1.12e+00   6.57e-02   16.98  0.00044 ***\nI(Height^2) -1.24e-03   1.38e-04   -8.99  0.00290 ** \nI(Height^3)  5.48e-07   8.33e-08    6.58  0.00715 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.01 on 3 degrees of freedom\nMultiple R-squared:  0.999, Adjusted R-squared:  0.999 \nF-statistic: 1.6e+03 on 3 and 3 DF,  p-value: 2.66e-05\n\n\nWe next decrease the polynomial for Height by one degree to obtain a quadratic model as interpreted on page 273 and summarized in Display 10.7 (page 281). This model is used for most of the following results.\n\ngalileo_lm2 &lt;- lm(Distance ~ Height + I(Height^2), data = case1001)\nsummary(galileo_lm2)\n\n\nCall:\nlm(formula = Distance ~ Height + I(Height^2), data = case1001)\n\nResiduals:\n     1      2      3      4      5      6      7 \n-14.31   9.17  13.52   1.94  -6.18 -12.61   8.46 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.00e+02   1.68e+01   11.93  0.00028 ***\nHeight       7.08e-01   7.48e-02    9.47  0.00069 ***\nI(Height^2) -3.44e-04   6.68e-05   -5.15  0.00676 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.6 on 4 degrees of freedom\nMultiple R-squared:  0.99,  Adjusted R-squared:  0.986 \nF-statistic:  205 on 2 and 4 DF,  p-value: 9.33e-05\n\n\nThe following figure replicates Display 10.2 (page 273), displaying the fitted quadratic model.\n\ngf_point(Distance ~ Height, data = case1001) %&gt;%\n  gf_lm(formula = y ~ x + I(x^2)) %&gt;%\n  gf_labs(x = \"Horizontal distance (punti)\", y = \"Initial height (punti)\")\n\nWarning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\n\nTo obtain the expected values of \\(\\widehat{\\mu}({\\rm Distance}|{\\rm Height} = 0)\\) and \\(\\widehat{\\mu}({\\rm Distance}|{\\rm Height} = 250)\\), we use the predict() command with the quadratic model as shown in Display 10.7 (page 281).\n\npredict(galileo_lm2, newdata = data.frame(Height = c(0, 250)), interval = \"confidence\")\n\n    fit   lwr   upr\n1 199.9 153.4 246.4\n2 355.5 337.1 373.9\n\n\nWe can also verify the above confidence interval calculations with the following code:\n\n# Obtain the fitted value and SE\npredict(galileo_lm2, newdata = data.frame(Height = c(0, 250)), interval = \"confidence\", se.fit = TRUE)\n\n$fit\n    fit   lwr   upr\n1 199.9 153.4 246.4\n2 355.5 337.1 373.9\n\n$se.fit\n     1      2 \n16.759  6.625 \n\n$df\n[1] 4\n\n$residual.scale\n[1] 13.64\n\n# \"by-hand\" calculation\n355.5 + c(-1, 1) * qt(0.975, df = 4) * 6.625 \n\n[1] 337.1 373.9\n\n\nTo verify numbers on page 284, an interval for the predicted values , we used the following code:\n\npredict(galileo_lm2, newdata = data.frame(Height = c(0, 250)), interval = \"predict\")\n\n    fit   lwr   upr\n1 199.9 139.9 259.9\n2 355.5 313.4 397.6\n\n\nLastly, we produced an ANOVA for the quadratic model interpreted on page 288 (Display 10.11). Notice that to get the entries for the “Regression” row on Display 10.11, you need to add the entries from the first two rows. This is because R displays sequential sums of squares and degrees of freedom. (Caution: Only the last p-value is valid!)\n\nanova(galileo_lm2)\n\nAnalysis of Variance Table\n\nResponse: Distance\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nHeight       1  71351   71351   383.6  4e-05 ***\nI(Height^2)  1   4927    4927    26.5 0.0068 ** \nResiduals    4    744     186                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferential Tools for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapter10.html#data-coding-summary-statistics-and-graphical-display-1",
    "href": "chapter10.html#data-coding-summary-statistics-and-graphical-display-1",
    "title": "6  Inferential Tools for Multiple Regression",
    "section": "8.1 Data coding, summary statistics and graphical display",
    "text": "8.1 Data coding, summary statistics and graphical display\nWe begin by reading the data, performing transformations where necessary and summarizing the variables.\n\ncase1002$Type &lt;-factor(case1002$Type, \n                       levels = c(\"non-echolocating bats\", \"non-echolocating birds\", \n                                  \"echolocating bats\"))\nsummary(case1002)\n\n      Mass                           Type        Energy     \n Min.   :  6.7   non-echolocating bats : 4   Min.   : 1.02  \n 1st Qu.: 63.4   non-echolocating birds:12   1st Qu.: 7.61  \n Median :266.5   echolocating bats     : 4   Median :22.60  \n Mean   :262.7                               Mean   :19.52  \n 3rd Qu.:391.0                               3rd Qu.:28.23  \n Max.   :779.0                               Max.   :43.70  \n\n\nA total of 20 flying vertebrates were included in this study. There were 4 echolocating bats, 4 non-echolocating bats, and 12 non-echolocating birds. For each subject their mass and flight energy expenditure were recorded as shown in Display 10.3 (page 274).\nNext, we explore the relationship between log(energy expenditure) as a function of log(body mass) for each group with a scatterplot. The following figure reproduces Display 10.4 (page 275).\n\ngf_point(log(Energy) ~ log(Mass), data = case1002, shape = ~Type) %&gt;%\n  gf_labs(x = \"Body Mass (g) (log scale)\", y = \"Energy Expenditure (W) (log scale)\") %&gt;%\n  gf_theme(legend.position=c(.2, .9)) # 0,0 is the bottom left; 1,1 is the top right",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferential Tools for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapter10.html#multiple-regression",
    "href": "chapter10.html#multiple-regression",
    "title": "6  Inferential Tools for Multiple Regression",
    "section": "8.2 Multiple regression",
    "text": "8.2 Multiple regression\nWe first evaluate a multiple regression model for log(energy expenditure) given type of species and log(body mass) as defined on page 276 and shown in Display 10.6 (page 277).\n\nenergy_lm1 &lt;- lm(log(Energy) ~ log(Mass) + Type, data = case1002)\nsummary(energy_lm1)\n\n\nCall:\nlm(formula = log(Energy) ~ log(Mass) + Type, data = case1002)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2322 -0.1220 -0.0364  0.1257  0.3446 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 -1.5764     0.2872   -5.49  5.0e-05 ***\nlog(Mass)                    0.8150     0.0445   18.30  3.8e-12 ***\nTypenon-echolocating birds   0.1023     0.1142    0.90     0.38    \nTypeecholocating bats        0.0787     0.2027    0.39     0.70    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.186 on 16 degrees of freedom\nMultiple R-squared:  0.982, Adjusted R-squared:  0.978 \nF-statistic:  284 on 3 and 16 DF,  p-value: 4.46e-14\n\n\nNext, we calculate confidence intervals for the coefficients that are interpreted on page 278.\n\nconfint(energy_lm1)\n\n                             2.5 %  97.5 %\n(Intercept)                -2.1853 -0.9674\nlog(Mass)                   0.7205  0.9094\nTypenon-echolocating birds -0.1398  0.3443\nTypeecholocating bats      -0.3510  0.5083\n\nexp(confint(energy_lm1))\n\n                            2.5 % 97.5 %\n(Intercept)                0.1124 0.3801\nlog(Mass)                  2.0555 2.4828\nTypenon-echolocating birds 0.8695 1.4110\nTypeecholocating bats      0.7040 1.6625\n\n\nSince the significance of a model depends on which variables are included, Sleuth proposes two other models, one only looking at the type of flying animal and the other allows the three groups to have different straight-line regressions with mass. These two models are displayed below and discussed on pages 278-279.\n\nenergy_lm2 &lt;- lm(log(Energy) ~ Type, data = case1002)\nenergy_lm3 &lt;- lm(log(Energy) ~ log(Mass) * Type, data = case1002)\n\nTable 1 presents the models displays on page 279 using stargazer.\nstargazer(energy_lm2, energy_lm1, energy_lm3, type = \"latex\", digits = 2, \n          omit.table.layout = \"sn\", \n          title = \"Coefficients and standard errors for the three models on pare 279 in Sleuth.\", \n          header=FALSE, \n          covariate.labels = c(\"lmass\", \"bird\", \"ebat\", \"bird:lmass\", \"ebat:lmass\"))\nNext we can assess the model by evaluating the extra sums of squares F-test for testing the equality of intercepts in the parallel regression lines model as shown in Display 10.10 (page 287).\n\nenergy_slr &lt;- lm(log(Energy) ~ log(Mass), data = case1002)\nanova(energy_slr, energy_lm1)\n\nAnalysis of Variance Table\n\nModel 1: log(Energy) ~ log(Mass)\nModel 2: log(Energy) ~ log(Mass) + Type\n  Res.Df   RSS Df Sum of Sq    F Pr(&gt;F)\n1     18 0.583                         \n2     16 0.553  2    0.0296 0.43   0.66\n\n\nWe can also compare the full model with interaction terms and the reduced model (without interaction terms) with the extra sum of squares F-test as described in Display 10.12 (page 290).\n\nanova(energy_lm1, energy_lm3)\n\nAnalysis of Variance Table\n\nModel 1: log(Energy) ~ log(Mass) + Type\nModel 2: log(Energy) ~ log(Mass) * Type\n  Res.Df   RSS Df Sum of Sq    F Pr(&gt;F)\n1     16 0.553                         \n2     14 0.505  2    0.0484 0.67   0.53",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferential Tools for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapter10.html#linear-combinations-of-regression-coefficients",
    "href": "chapter10.html#linear-combinations-of-regression-coefficients",
    "title": "6  Inferential Tools for Multiple Regression",
    "section": "8.3 Linear combinations of regression coefficients",
    "text": "8.3 Linear combinations of regression coefficients\nFinally, we can obtain the standard error for \\(\\beta_2 - \\beta_3\\), as detailed on Display 10.15 (page 294), using the below commands:\n\n# 1. estimate the linear combination\nestimate &lt;- coef(energy_lm1)[3] - coef(energy_lm1)[4]\nestimate\n\nTypenon-echolocating birds \n                    0.0236 \n\n# 2. Obtain the variance-covariance matrix of the coefficients\nV &lt;- vcov(energy_lm1)\nV\n\n                           (Intercept) log(Mass) Typenon-echolocating birds\n(Intercept)                    0.08250 -0.012105                  -0.019207\nlog(Mass)                     -0.01211  0.001984                   0.001731\nTypenon-echolocating birds    -0.01921  0.001731                   0.013038\nTypeecholocating bats         -0.05056  0.006870                   0.014639\n                           Typeecholocating bats\n(Intercept)                             -0.05056\nlog(Mass)                                0.00687\nTypenon-echolocating birds               0.01464\nTypeecholocating bats                    0.04108\n\n# 3. Calculate the estimated variance of the linear combination\nv.b2 &lt;- V[3, 3]\nv.b2\n\n[1] 0.01304\n\nv.b3 &lt;- V[4, 4]\nv.b3\n\n[1] 0.04108\n\ncov.b2b3 &lt;- V[3, 4]\ncov.b2b3\n\n[1] 0.01464\n\nestimated_var &lt;- v.b2 + v.b3 - 2 * cov.b2b3\nestimated_var\n\n[1] 0.02484\n\nse &lt;- sqrt(estimated_var)\nse\n\n[1] 0.1576",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferential Tools for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapter11.html",
    "href": "chapter11.html",
    "title": "7  Model Checking and Refinement",
    "section": "",
    "text": "8 Alcohol metabolism in men and women\nIn this chapter we need to load the following packages (remember, you will need to install packages you have never used before if you are using your own computer).\nWe will also set some options to improve legibility of graphs and output.\nHow do men and women metabolize alcohol? This is the question addressed in case study 11.1 in Sleuth.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Checking and Refinement</span>"
    ]
  },
  {
    "objectID": "chapter11.html#data-coding-summary-statistics-and-graphical-display",
    "href": "chapter11.html#data-coding-summary-statistics-and-graphical-display",
    "title": "7  Model Checking and Refinement",
    "section": "8.1 Data coding, summary statistics and graphical display",
    "text": "8.1 Data coding, summary statistics and graphical display\nWe begin by loading the data and summarizing the variables\n\nsummary(case1101)\n\n    Subject         Metabol         Gastric         Sex              Alcohol  \n Min.   : 1.00   Min.   : 0.10   Min.   :0.80   Female:18   Alcoholic    : 8  \n 1st Qu.: 8.75   1st Qu.: 0.60   1st Qu.:1.20   Male  :14   Non-alcoholic:24  \n Median :16.50   Median : 1.70   Median :1.60                                 \n Mean   :16.50   Mean   : 2.42   Mean   :1.86                                 \n 3rd Qu.:24.25   3rd Qu.: 2.92   3rd Qu.:2.20                                 \n Max.   :32.00   Max.   :12.30   Max.   :5.20                                 \n\n\nA total of 32 volunteers were included in this data. There were 18 females and 14 males, as recorded on Display 11.1 (page 311 of the Sleuth).\nThe following is a version of Display 11.2 (page 312):\n\ngf_point(Metabol ~ Gastric, data = case1101, shape = ~Alcohol, color = ~Sex) %&gt;%\n  gf_labs(x = expression(paste(\"Gastric AD activity (\", mu, \"mol/min/g of tissue)\", sep = \"\")), \n          y = \"First-pass metabolism (mmol/liter-hour)\") %&gt;%\n  gf_refine(scale_color_brewer(palette = \"Dark2\"), theme(legend.position = \"top\"))\n\n\n\n\n\n\n\n\nThe above plot uses shape and color to encode different information, which can be harder to read than necessary. A better idea is to use color and shape to encode the same information and to use faceting to introduce another variable:\n\ngf_point(Metabol ~ Gastric | Sex, data = case1101, shape = ~Alcohol, color = ~Alcohol) %&gt;%\n  gf_labs(x = expression(paste(\"Gastric AD activity (\", mu, \"mol/min/g of tissue)\", sep = \"\")), \n          y = \"First-pass metabolism (mmol/liter-hour)\") %&gt;%\n  gf_refine(scale_color_brewer(palette = \"Dark2\"), theme(legend.position = \"top\"))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Checking and Refinement</span>"
    ]
  },
  {
    "objectID": "chapter11.html#multiple-regression",
    "href": "chapter11.html#multiple-regression",
    "title": "7  Model Checking and Refinement",
    "section": "8.2 Multiple regression",
    "text": "8.2 Multiple regression\nNext, we fit the full model (including all pairwise interactions) for estimating metabolism given a subjects gastric AD activity, whether they are alcoholic, and their sex. This first model is summarized on page 321 (Display 11.9). Notice that the baseline levels of Sex and Alcohol are redefined prior to modeling.\n\ncase1101 &lt;- case1101 %&gt;%\n  mutate(Sex = factor(Sex, levels = c(\"Male\", \"Female\")),\n         Alcohol = factor(Alcohol, levels = c(\"Non-alcoholic\", \"Alcoholic\")))\n\ncase1_mod1 &lt;- lm(Metabol ~ Gastric * Sex * Alcohol, data = case1101)\ntidy(case1_mod1)\n\n# A tibble: 8 × 5\n  term                               estimate std.error statistic     p.value\n  &lt;chr&gt;                                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)                           -1.66     1.00     -1.66  0.110      \n2 Gastric                                2.51     0.343     7.32  0.000000146\n3 SexFemale                              1.47     1.33      1.10  0.282      \n4 AlcoholAlcoholic                       2.55     1.95      1.31  0.202      \n5 Gastric:SexFemale                     -1.67     0.620    -2.70  0.0126     \n6 Gastric:AlcoholAlcoholic              -1.46     1.05     -1.39  0.179      \n7 SexFemale:AlcoholAlcoholic            -2.25     4.39     -0.512 0.613      \n8 Gastric:SexFemale:AlcoholAlcoholic     1.20     3.00      0.400 0.693      \n\n\nNotice also that the asterisk (*) is used to code interactions in R. When an interaction is specified in this way, R will fit all of the terms involved in the largest interaction. For example, in the above model formula we specified Gastric * Sex * Alcohol which resulted in all of the main effects (Gastric Sex, Alcohol), pairwise interactions (Gastric:Sex, Gastric:Alcohol, and Sex:Alcohol), and the triplet (Gastric:Sex:Alcohol) being included. The above model can be fit using the equivalent “long-hand” formula:\n\ncase1_mod1 &lt;- lm(Metabol ~ Gastric + Sex + Alcohol + Gastric * Sex + Sex * Alcohol + Gastric * Alcohol + \n                   Gastric * Sex * Alcohol, data = case1101)\ntidy(case1_mod1)\n\n# A tibble: 8 × 5\n  term                               estimate std.error statistic     p.value\n  &lt;chr&gt;                                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)                           -1.66     1.00     -1.66  0.110      \n2 Gastric                                2.51     0.343     7.32  0.000000146\n3 SexFemale                              1.47     1.33      1.10  0.282      \n4 AlcoholAlcoholic                       2.55     1.95      1.31  0.202      \n5 Gastric:SexFemale                     -1.67     0.620    -2.70  0.0126     \n6 SexFemale:AlcoholAlcoholic            -2.25     4.39     -0.512 0.613      \n7 Gastric:AlcoholAlcoholic              -1.46     1.05     -1.39  0.179      \n8 Gastric:SexFemale:AlcoholAlcoholic     1.20     3.00      0.400 0.693      \n\n\nHaving fit the full model, we calculate a number of model diagnostics, including leverage, studentized (i.e. standardized) residuals, and Cook’s distance (pages 325-327).\n\ncase1_aug &lt;- augment(case1_mod1)\n\nThe 31st row of this augmented data frame is shown below:\n\ncase1_aug[31,]\n\n# A tibble: 1 × 10\n  Metabol Gastric Sex   Alcohol       .fitted .resid  .hat .sigma .cooksd .std.resid\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;           &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1     9.5     5.2 Male  Non-alcoholic    11.4  -1.91 0.601   1.11    1.10      -2.41\n\n\nNote that in this augmented data frame leverage is called .hat, studentized (standardized) residuals are called .std.resid, and Cook’s distances are called .cooksd.\nIt is often easier to digest these values visually. The infIndexPlot() command in the car package allows us to do this just as in Display 11.12 (page 327):\n\ninfIndexPlot(case1_mod1)\n\n\n\n\n\n\n\n\nNote: the “Bonferroni p-value” panel in the plot is displaying the results for a test of outlierhood.\nAs pointed out in Sleuth, cases 31 and 32 have substantially larger studentized residuals than the other observations. Interestingly, observations 1 and 23 stand out more than 31 and 32 in our version of the index plots.\nThe authors of Sleuth refit the model without observations 31 and 32.\n\ncase1_mod2 &lt;- lm(Metabol ~ Gastric + Sex + Alcohol + Gastric * Sex + Sex * Alcohol + Gastric * Alcohol + Gastric * Sex * Alcohol, data = case1101, subset = -c(31, 32))\n\nA table comparing the model coefficients with and without cases 21 and 32 is shown below:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Checking and Refinement</span>"
    ]
  },
  {
    "objectID": "chapter11.html#refining-the-model",
    "href": "chapter11.html#refining-the-model",
    "title": "7  Model Checking and Refinement",
    "section": "8.3 Refining the Model",
    "text": "8.3 Refining the Model\nThis section addresses the process of refining the model. First, we use an extra-sums-of-square F-test to determine whether the terms involving alcoholism can be removed from the full model, as shown in Display 11.13 (page 329).\n\ncase1_mod3 &lt;- lm(Metabol ~ Gastric * Sex, data = case1101, subset = -c(31, 32))\nanova(case1_mod3, case1_mod2)\n\nAnalysis of Variance Table\n\nModel 1: Metabol ~ Gastric * Sex\nModel 2: Metabol ~ Gastric + Sex + Alcohol + Gastric * Sex + Sex * Alcohol + \n    Gastric * Alcohol + Gastric * Sex * Alcohol\n  Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)\n1     26 20.2                         \n2     22 19.5  4      0.74 0.21   0.93\n\n\nAs noted in Sleuth there is no evidence of an alcoholism effect, so we can utilize the smaller model. There are no issue revealed by inspection of the residual plots. Further, revisiting the case influence diagnostics, we now see that no observations stand out. (Note: the issues with cases 1 and 23 previously seen were due to the fact that there were so few alcoholics in the data set, so it was “easy” for points to be potentially influential due to data sparsity.)\n\nresidualPlots(case1_mod3, layout = c(1,3), tests = FALSE)\n\n\n\n\n\n\n\n\n\ninfIndexPlot(case1_mod3)\n\n\n\n\n\n\n\n\nNext, we assess a model without an intercept, which is scientifically plausible and summarized in Display 11.14 (page 329).\n\ncase1_mod4 &lt;- lm(Metabol ~ Gastric + Gastric:Sex - 1, data = case1101, subset = -c(31, 32))\ntidy(case1_mod4)\n\n# A tibble: 3 × 5\n  term              estimate std.error statistic     p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 Gastric              0.726     0.121      5.99  0.00000187\n2 Gastric:SexMale      0.873     0.174      5.02  0.0000263 \n3 Gastric:SexFemale   NA        NA         NA    NA         \n\n\n\nanova(case1_mod4, case1_mod3)\n\nAnalysis of Variance Table\n\nModel 1: Metabol ~ Gastric + Gastric:Sex - 1\nModel 2: Metabol ~ Gastric * Sex\n  Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)\n1     28 20.3                         \n2     26 20.2  2     0.094 0.06   0.94\n\n\nNote that the “Summary of Statistical Findings” section (page 312) is based on this final model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Checking and Refinement</span>"
    ]
  },
  {
    "objectID": "chapter11.html#data-coding-and-summary-statistics",
    "href": "chapter11.html#data-coding-and-summary-statistics",
    "title": "7  Model Checking and Refinement",
    "section": "9.1 Data coding and summary statistics",
    "text": "9.1 Data coding and summary statistics\nWe begin by loading the data, performing transformations where needed, and summarizing the variables.\n\nnames(case1102)\n\n[1] \"Brain\"     \"Liver\"     \"Time\"      \"Treatment\" \"Days\"      \"Sex\"       \"Weight\"   \n[8] \"Loss\"      \"Tumor\"    \n\ncase1102 &lt;- case1102 %&gt;%\n  mutate(Y = Brain/Liver,\n         SAC = as.factor(Time),\n         logliver = log(Liver),\n         logbrain = log(Brain),\n         logy = log(Brain/Liver),\n         logtime = log(Time),\n         Sex = relevel(Sex, ref = \"Male\"),\n         Treat = relevel(Treatment, ref = \"NS\")) # make \"NS\" referenec level\n\nsummary(case1102)\n\n     Brain            Liver              Time       Treatment      Days        Sex    \n Min.   :  1334   Min.   :    928   Min.   : 0.50   BD:17     Min.   : 9   Male  : 8  \n 1st Qu.: 19281   1st Qu.:  16210   1st Qu.: 1.12   NS:17     1st Qu.:10   Female:26  \n Median : 32572   Median : 643965   Median : 3.00             Median :10              \n Mean   : 39965   Mean   : 668776   Mean   :23.51             Mean   :10              \n 3rd Qu.: 50654   3rd Qu.:1318557   3rd Qu.:24.00             3rd Qu.:10              \n Max.   :123730   Max.   :1790863   Max.   :72.00             Max.   :11              \n     Weight         Loss           Tumor           Y           SAC       logliver    \n Min.   :184   Min.   :-4.90   Min.   : 25   Min.   :0.0103   0.5:9   Min.   : 6.83  \n 1st Qu.:225   1st Qu.: 1.20   1st Qu.:136   1st Qu.:0.0337   3  :9   1st Qu.: 9.69  \n Median :240   Median : 3.95   Median :166   Median :0.1193   24 :8   Median :13.37  \n Mean   :242   Mean   : 3.64   Mean   :183   Mean   :1.5036   72 :8   Mean   :11.61  \n 3rd Qu.:259   3rd Qu.: 5.97   3rd Qu.:223   3rd Qu.:1.9516           3rd Qu.:14.09  \n Max.   :298   Max.   :12.80   Max.   :484   Max.   :8.5454           Max.   :14.40  \n    logbrain          logy           logtime       Treat  \n Min.   : 7.20   Min.   :-4.578   Min.   :-0.693   NS:17  \n 1st Qu.: 9.86   1st Qu.:-3.390   1st Qu.:-0.245   BD:17  \n Median :10.39   Median :-2.126   Median : 1.099          \n Mean   :10.23   Mean   :-1.389   Mean   : 1.861          \n 3rd Qu.:10.83   3rd Qu.: 0.668   3rd Qu.: 3.178          \n Max.   :11.73   Max.   : 2.145   Max.   : 4.277          \n\n\nA total of 34 rats were included in this experiment. Each rat was given either the barrier solution (\\(n = 17\\)) or a normal saline solution (\\(n = 17\\)). Then variables of interest were calculated and are displayed in Display 11.4 (page 314 of the Sleuth).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Checking and Refinement</span>"
    ]
  },
  {
    "objectID": "chapter11.html#graphical-presentation",
    "href": "chapter11.html#graphical-presentation",
    "title": "7  Model Checking and Refinement",
    "section": "9.2 Graphical presentation",
    "text": "9.2 Graphical presentation\nA scatterplot of log ratio (Y) as a function of log time, as shown in Display 11.5 on page 315, is shown below.\n\ngf_point(Y ~ Time, data = case1102, shape = ~Treat) %&gt;%\n  gf_refine(coord_trans(x = \"log10\", y=\"log10\"),\n            scale_shape_manual(values = c(1, 16))) %&gt;% # 1 = hollow, 16 = filled\n  gf_labs(x = \"Sacrifice Time (hours)\", y = \"Tumor-to-Liver Concentration Ratio\")\n\n\n\n\n\n\n\n\nWe can also graphically explore relationships between the variables using a scatterplot matrix.\n\nscatterplotMatrix(~ logy + logbrain + logliver + Treat + SAC, smooth = FALSE , data = case1102)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Checking and Refinement</span>"
    ]
  },
  {
    "objectID": "chapter11.html#multiple-regression-1",
    "href": "chapter11.html#multiple-regression-1",
    "title": "7  Model Checking and Refinement",
    "section": "9.3 Multiple regression",
    "text": "9.3 Multiple regression\nFirst, we fit the model proposed on page 317.\n\ncase2_mod1 &lt;- lm(logy ~ SAC * Treat + Days + Sex + Weight + Loss + Tumor, data = case1102)\ntidy(case2_mod1)\n\n# A tibble: 13 × 5\n   term           estimate std.error statistic  p.value\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)   -3.84       3.37     -1.14    2.68e- 1\n 2 SAC3           1.02       0.400     2.54    1.90e- 2\n 3 SAC24          4.34       0.478     9.08    1.03e- 8\n 4 SAC72          5.01       0.455    11.0     3.48e-10\n 5 TreatBD        0.796      0.379     2.10    4.79e- 2\n 6 Days          -0.0370     0.296    -0.125   9.02e- 1\n 7 SexFemale     -0.00130    0.373    -0.00347 9.97e- 1\n 8 Weight        -0.000558   0.00533  -0.105   9.18e- 1\n 9 Loss          -0.0595     0.0304   -1.96    6.37e- 2\n10 Tumor          0.00155    0.00123   1.26    2.20e- 1\n11 SAC3:TreatBD   0.180      0.552     0.326   7.48e- 1\n12 SAC24:TreatBD -0.386      0.585    -0.659   5.17e- 1\n13 SAC72:TreatBD  0.379      0.569     0.666   5.13e- 1\n\n\nWe can then display a residual plot to assess the fit of the above model. This is provided in Display 11.6 (page 318). Note that id.n prints the row numbers for the n largest residuals.\n\nresidualPlot(case2_mod1, id = list(n = 2))\n\n\n\n\n\n\n\n\nNext, we can use partial residual plots to examine the effects of the covariates (Sex and Days) after the design variables (sacrifice time and treatment) have been accounted for. To do this it is tempting to use partial residual plots, however, as defined in Sleuth, partial residuals cannot be calculated for terms involved in interactions, since it is impossible to isolate the effect of the term after accounting for the others (namely, the term(s) involved in the interaction(s)). To reproduce Display 11.16, we would need to determine exactly what the author’s calculated. (My guess is that they fit the model without interactions if they followed their own definition.) See the last section of this packet for a reproduction of Display 11.15.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Checking and Refinement</span>"
    ]
  },
  {
    "objectID": "chapter11.html#refining-the-model-1",
    "href": "chapter11.html#refining-the-model-1",
    "title": "7  Model Checking and Refinement",
    "section": "9.4 Refining the model",
    "text": "9.4 Refining the model\nFinally, we fit a refined model. These results can be found in Display 11.17 (page 334).\n\ncase2_mod2 &lt;- lm(logy ~ SAC + Treat, data = case1102)\ntidy(case2_mod2)\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -4.30      0.205    -21.0  4.30e-19\n2 SAC3           1.13      0.252      4.50 1.01e- 4\n3 SAC24          4.26      0.259     16.4  3.13e-16\n4 SAC72          5.15      0.259     19.9  1.90e-18\n5 TreatBD        0.797     0.183      4.35 1.55e- 4\n\nanova(case2_mod2, case2_mod1)\n\nAnalysis of Variance Table\n\nModel 1: logy ~ SAC + Treat\nModel 2: logy ~ SAC * Treat + Days + Sex + Weight + Loss + Tumor\n  Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)\n1     29 8.23                         \n2     21 6.68  8      1.55 0.61   0.76",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Checking and Refinement</span>"
    ]
  },
  {
    "objectID": "chapter12.html",
    "href": "chapter12.html",
    "title": "8  Strategies for Variable Selection",
    "section": "",
    "text": "9 State Average SAT Scores\nIn this chapter we need to load the following packages (remember, you will need to install packages you have never used before if you are using your own computer).\nWe will also set some options to improve legibility of graphs and output.\nWhat variables are associated with state SAT scores? This is the question addressed in case study 12.1 in the Sleuth.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Strategies for Variable Selection</span>"
    ]
  },
  {
    "objectID": "chapter12.html#summary-statistics",
    "href": "chapter12.html#summary-statistics",
    "title": "8  Strategies for Variable Selection",
    "section": "9.1 Summary statistics",
    "text": "9.1 Summary statistics\nWe begin by loading the data and summarizing the variables\n\nsummary(case1201)\n\n        State         SAT           Takers          Income        Years     \n Alabama   : 1   Min.   : 790   Min.   : 2.00   Min.   :208   Min.   :14.4  \n Alaska    : 1   1st Qu.: 889   1st Qu.: 6.25   1st Qu.:262   1st Qu.:15.9  \n Arizona   : 1   Median : 966   Median :16.00   Median :295   Median :16.4  \n Arkansas  : 1   Mean   : 948   Mean   :26.22   Mean   :294   Mean   :16.2  \n California: 1   3rd Qu.: 998   3rd Qu.:47.75   3rd Qu.:325   3rd Qu.:16.8  \n Colorado  : 1   Max.   :1088   Max.   :69.00   Max.   :401   Max.   :17.4  \n (Other)   :44                                                              \n     Public         Expend          Rank     \n Min.   :44.8   Min.   :13.8   Min.   :69.8  \n 1st Qu.:76.9   1st Qu.:19.6   1st Qu.:74.0  \n Median :80.8   Median :21.6   Median :80.8  \n Mean   :81.2   Mean   :23.0   Mean   :80.0  \n 3rd Qu.:88.2   3rd Qu.:26.4   3rd Qu.:85.8  \n Max.   :97.0   Max.   :50.1   Max.   :90.6  \n                                             \n\n\nThe data are shown on page 347 (display 12.1). A total of 50 state average SAT scores are included in this data set.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Strategies for Variable Selection</span>"
    ]
  },
  {
    "objectID": "chapter12.html#dealing-with-many-explanatory-variables",
    "href": "chapter12.html#dealing-with-many-explanatory-variables",
    "title": "8  Strategies for Variable Selection",
    "section": "9.2 Dealing with Many Explanatory Variables",
    "text": "9.2 Dealing with Many Explanatory Variables\nThe following graph is presented as Display 12.4 (page 356).\n\npairs(~Takers + Rank + Years + Income + Public + Expend + SAT, data = case1201)\n\n\n\n\n\n\n\n\nUsing the car or GGally packages you can produce “fancier” versions of this scatterplot matrix:\n\n# Using the car package\nscatterplotMatrix(~Takers + Rank + Years + Income + Public + Expend + SAT, \n                  diagonal=list(method =\"histogram\"), smooth = FALSE, data = case1201)\n\n\n\n\n\n\n\n\n\n# Using the GGally package\nlibrary(GGally)\nggpairs(columns = c(\"Takers\", \"Rank\", \"Years\", \"Income\", \"Public\", \"Expend\", \"SAT\"), data = case1201)\n\n\n\n\n\n\n\n\nBased on the scatterplot matrix, we choose to transform the percentage of SAT takers using the (natural) logarithm. An initial model is then fit using the transformed percentage of SAT takers and median class rank (page 355-357):\n\ncase1_mod1 &lt;- lm(SAT ~ Rank + log(Takers), data = case1201)\nsummary(case1_mod1)\n\n\nCall:\nlm(formula = SAT ~ Rank + log(Takers), data = case1201)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-94.46 -17.31   5.32  22.82  48.47 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   882.08     224.13    3.94  0.00027\nRank            2.40       2.33    1.03  0.30898\nlog(Takers)   -45.19      14.06   -3.21  0.00236\n\nResidual standard error: 31.1 on 47 degrees of freedom\nMultiple R-squared:  0.815, Adjusted R-squared:  0.807 \nF-statistic:  103 on 2 and 47 DF,  p-value: &lt;2e-16\n\n\nFrom the regression output, we observe that these two variables can explain 81.5% of the variation in SAT scores.\nNext we fit a linear regression model using all variables and create the partial residual plot presented as Display 12.5 (page 357).\n\ncase1_mod2 &lt;- lm(SAT ~ log(Takers) + Income + Years + Public + Expend + Rank, data = case1201)\nsummary(case1_mod2)\n\n\nCall:\nlm(formula = SAT ~ log(Takers) + Income + Years + Public + Expend + \n    Rank, data = case1201)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-61.11  -8.60   2.86  14.77  53.40 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 407.5399   282.7633    1.44   0.1567\nlog(Takers) -38.4376    15.9521   -2.41   0.0203\nIncome       -0.0359     0.1301   -0.28   0.7841\nYears        17.2181     6.3201    2.72   0.0093\nPublic       -0.1130     0.5624   -0.20   0.8417\nExpend        2.5669     0.8064    3.18   0.0027\nRank          4.1143     2.5017    1.64   0.1073\n\nResidual standard error: 24.9 on 43 degrees of freedom\nMultiple R-squared:  0.892, Adjusted R-squared:  0.877 \nF-statistic: 59.2 on 6 and 43 DF,  p-value: &lt;2e-16\n\n\nAccording to the Cook’s distance plot, obs 29 (Alaska) seems to be an influential outlier. We may consider removing this observation from the data set.\n\ninfIndexPlot(case1_mod2, vars = c(\"Cook\", \"Studentized\", \"hat\"))\n\n\n\n\n\n\n\n\nSleuth compares the coefficient for Expend with and without Alaska in the data set using the partial residual plots in Display 12.5.\n\ncase1_mod2na &lt;- update(case1_mod2, subset = (State != \"Alaska\"))\ntidy(case1_mod2na)\n\n# A tibble: 7 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  288.      259.        1.11  0.274    \n2 log(Takers)  -30.2      14.7      -2.05  0.0462   \n3 Income         0.103     0.126     0.817 0.418    \n4 Years         13.1       5.88      2.23  0.0312   \n5 Public        -0.101     0.511    -0.198 0.844    \n6 Expend         3.94      0.849     4.64  0.0000340\n7 Rank           5.27      2.30      2.29  0.0269   \n\n\n\npar(mfrow = c(1,2))\ncrPlot(case1_mod2, variable = \"Expend\")\ncrPlot(case1_mod2na, variable = \"Expend\")\n\n\n\n\n\n\n\ninvisible(dev.off())\n\nThe difference between these two slopes indicates that Alaska is an influential observation. Following the analysis presented in the text, we remove Alaska from the data set.\n\ncase1201na &lt;- filter(case1201, State != \"Alaska\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Strategies for Variable Selection</span>"
    ]
  },
  {
    "objectID": "chapter12.html#sequential-variable-selection",
    "href": "chapter12.html#sequential-variable-selection",
    "title": "8  Strategies for Variable Selection",
    "section": "9.3 Sequential Variable Selection",
    "text": "9.3 Sequential Variable Selection\nThe book uses F-statistics as the criterion to perform the procedures of forward selection and backward elimination presented on page 359. As mentioned on page 359, the entire forward selection procedure required the fitting of only 16 of the 64 possible models presented on Display 12.6 (page 360). These 16 models utilized Expenditure and log(Takers) to predict SAT scores. Further, as mentioned on page 359, the entire backward selection procedure required the fitting of only 3 models of the 64 possible models. These 3 models used Year, Expenditure, Rank and log(Takers) to predict SAT scores.\nTo the best of our knowledge, there is not an automated solution in R to run step-wise selection or forward selection. You can implement these manually using the addterm() and dropterm() functions found in the MASS package. For example, starting with the full model outlined in Sleuth, we can take the first two steps in backward elimination using the F-test:\n\ndropterm(case1_mod2na, test = \"F\")\n\nSingle term deletions\n\nModel:\nSAT ~ log(Takers) + Income + Years + Public + Expend + Rank\n            Df Sum of Sq   RSS AIC F Value   Pr(F)\n&lt;none&gt;                   21397 312                \nlog(Takers)  1      2150 23547 315    4.22   0.046\nIncome       1       340 21737 311    0.67   0.418\nYears        1      2532 23928 315    4.97   0.031\nPublic       1        20 21417 310    0.04   0.844\nExpend       1     10964 32361 330   21.52 3.4e-05\nRank         1      2679 24076 316    5.26   0.027\n\n\nBased on the above summary, Public is the term with the smallest F-statistic, so we should delete it.\n\ndrop1_mod &lt;- update(case1_mod2na, . ~ . - Public)\ntidy(drop1_mod)\n\n# A tibble: 6 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  291.      256.         1.14 0.261    \n2 log(Takers)  -31.2      13.8       -2.26 0.0287   \n3 Income         0.113     0.113      1.01 0.319    \n4 Years         13.5       5.49       2.46 0.0180   \n5 Expend         3.87      0.774      5.00 0.0000100\n6 Rank           5.06      2.01       2.52 0.0155   \n\n\nNow we determine what (if anything) should be eliminated next:\n\ndropterm(drop1_mod, test = \"F\")\n\nSingle term deletions\n\nModel:\nSAT ~ log(Takers) + Income + Years + Expend + Rank\n            Df Sum of Sq   RSS AIC F Value Pr(F)\n&lt;none&gt;                   21417 310              \nlog(Takers)  1      2552 23968 313    5.12 0.029\nIncome       1       505 21922 309    1.01 0.319\nYears        1      3011 24428 314    6.05 0.018\nExpend       1     12465 33882 330   25.03 1e-05\nRank         1      3162 24578 315    6.35 0.016\n\n\nBased on the above summary, Income is the term with the smallest F-statistic, so we should delete it.\n\ndrop2_mod &lt;- update(drop1_mod, . ~ . - Income)\ntidy(drop2_mod)\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)   399.     232.         1.72 0.0929    \n2 log(Takers)   -38.1     11.9       -3.20 0.00257   \n3 Years          13.1      5.48       2.40 0.0207    \n4 Expend          4.00     0.764      5.23 0.00000452\n5 Rank            4.40     1.90       2.32 0.0252    \n\n\nThis is our stopping point, since all of the F-statistics are greater than 4:\n\ndropterm(drop2_mod, test = \"F\")\n\nSingle term deletions\n\nModel:\nSAT ~ log(Takers) + Years + Expend + Rank\n            Df Sum of Sq   RSS AIC F Value   Pr(F)\n&lt;none&gt;                   21922 309                \nlog(Takers)  1      5094 27016 317   10.22  0.0026\nYears        1      2870 24792 313    5.76  0.0207\nExpend       1     13620 35542 331   27.34 4.5e-06\nRank         1      2676 24598 313    5.37  0.0252\n\n\nWhen model selection is relatively guided (or the pool of variables is small) manual implementation is possible (albeit tedious). To automate the process we need to utilize either the AIC or BIC criteria for step-wise selection. Below, we demonstrate this procedure using AIC criterion.\nTo run forward selection, you need to start with some preliminary model. This could be the intercept-only model, but it seems reasonable to include one predictor to start. Here, we choose log(Taker) as our preliminary predictor because it has the largest F-value when we fitted case1201na as seen above. Further, we need to give the selection procedure an upper bound for complexity, here we use case1_mod2na.\n\n# Forward selection\nprelim_mod &lt;- lm(SAT ~ log(Takers), data = case1201na)\nstepAIC(prelim_mod, scope = list(upper = case1_mod2na, lower = ~1), \n  direction = \"forward\", trace = FALSE)$anova\n\nStepwise Model Path \nAnalysis of Deviance Table\n\nInitial Model:\nSAT ~ log(Takers)\n\nFinal Model:\nSAT ~ log(Takers) + Expend + Years + Rank\n\n      Step Df Deviance Resid. Df Resid. Dev AIC\n1                             47      46369 340\n2 + Expend  1    20523        46      25846 313\n3  + Years  1     1248        45      24598 313\n4   + Rank  1     2676        44      21922 309\n\n\nTo run backward elimination, we simply provide the richest model under consideration and specify the direction:\n\n# Backward Elimination\nstepAIC(case1_mod2na, direction=\"backward\", trace=FALSE)$anova\n\nStepwise Model Path \nAnalysis of Deviance Table\n\nInitial Model:\nSAT ~ log(Takers) + Income + Years + Public + Expend + Rank\n\nFinal Model:\nSAT ~ log(Takers) + Years + Expend + Rank\n\n      Step Df Deviance Resid. Df Resid. Dev AIC\n1                             42      21397 312\n2 - Public  1       20        43      21417 310\n3 - Income  1      505        44      21922 309\n\n\nTo run step-wise selection, we again specify the richest model under consideration and specify the direction:\n\n# Stepwise Regression\nstepAIC(case1_mod2na, direction=\"both\", trace=FALSE)$anova\n\nStepwise Model Path \nAnalysis of Deviance Table\n\nInitial Model:\nSAT ~ log(Takers) + Income + Years + Public + Expend + Rank\n\nFinal Model:\nSAT ~ log(Takers) + Years + Expend + Rank\n\n      Step Df Deviance Resid. Df Resid. Dev AIC\n1                             42      21397 312\n2 - Public  1       20        43      21417 310\n3 - Income  1      505        44      21922 309\n\n\nIn this case study, the final model includes log(Takers), Expenditure, Years, and Rank. The final model can explain 91.1% percent or the variation of SAT.\n\nfinal_mod &lt;- lm(SAT ~ log(Takers) + Years + Expend + Rank, case1201na)\nsummary(final_mod)\n\n\nCall:\nlm(formula = SAT ~ log(Takers) + Years + Expend + Rank, data = case1201na)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-52.30  -9.92   0.60  11.88  59.20 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  399.115    232.372    1.72   0.0929\nlog(Takers)  -38.100     11.915   -3.20   0.0026\nYears         13.147      5.478    2.40   0.0207\nExpend         3.996      0.764    5.23  4.5e-06\nRank           4.400      1.899    2.32   0.0252\n\nResidual standard error: 22.3 on 44 degrees of freedom\nMultiple R-squared:  0.911, Adjusted R-squared:  0.903 \nF-statistic:  112 on 4 and 44 DF,  p-value: &lt;2e-16\n\n\nNote, the three procedures do not always agree, so you should think of the results as a few competing models to further explore.\nAdditionally, remember that the full (i.e. richest) model should be checked for deficiencies and multicollinearity prior to being used with model selection.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Strategies for Variable Selection</span>"
    ]
  },
  {
    "objectID": "chapter12.html#model-selection-among-all-subsets",
    "href": "chapter12.html#model-selection-among-all-subsets",
    "title": "8  Strategies for Variable Selection",
    "section": "9.4 Model Selection Among All Subsets",
    "text": "9.4 Model Selection Among All Subsets\nThe \\(C_p\\)-statistic can be an useful criterion to select model among all subsets. Here, we give an example about how to calculate this statistic for one model, which includes log(Takers), Expenditure, Years and Rank.\n\nsigma_final &lt;- summary(final_mod)$sigma^2 # sigma-squared of chosen model\nsigma_full &lt;- summary(case1_mod2na)$sigma^2   # sigma-squared of full model\nn &lt;- 49  # sample size\np &lt;- 4 + 1 # number of coefficients in model\nCp &lt;- (n - p) * sigma_final / sigma_full + (2 * p - n)\nCp\n\n[1] 4.03\n\n\nAIC and BIC are also useful criteria and each can be easily calculated for any fitted regression model. Below we calculate AIC and BIC for case1_mod2na:\n\nAIC(case1_mod2na)\n\n[1] 453\n\nBIC(case1_mod2na)\n\n[1] 468\n\n\nWhile it can be useful to calculate these criteria for a fitted model, to conduct all-subsets model selection we turn to the implementation in the leaps package. To run model selection through an exhaustive search of all models we can use the regsubsets command.\n\npredictors &lt;- dplyr::select(case1201na, -State, -SAT)\nresponse &lt;- case1201na$SAT\nallsubsets_results &lt;- regsubsets(x = predictors, y = response, method = \"exhaustive\")\nallsubsets_results\n\nSubset selection object\n6 Variables  (and intercept)\n       Forced in Forced out\nTakers     FALSE      FALSE\nIncome     FALSE      FALSE\nYears      FALSE      FALSE\nPublic     FALSE      FALSE\nExpend     FALSE      FALSE\nRank       FALSE      FALSE\n1 subsets of each size up to 6\nSelection Algorithm: exhaustive\n\n\nBy default, regsubsets returns information about the top performing model of each size. To return the top n models of each size, add the argument nbest = n (where you specify n) to the call. To access the values of BIC and \\(C_p\\), first run summary:\n\n# Calculate the summary of the selection\ntop_models &lt;- summary(allsubsets_results)\ntop_models$cp  # print Cp\n\n[1] 53.25 22.67  7.19  3.75  5.00  7.00\n\ntop_models$bic # print BIC\n\n[1] -65.4 -81.3 -92.5 -94.5 -91.5 -87.6\n\n\nAlternatively, you can plot the results:\n\npar(mfrow = c(1,2))\nplot(allsubsets_results, scale = \"bic\")\nplot(allsubsets_results, scale = \"Cp\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Strategies for Variable Selection</span>"
    ]
  },
  {
    "objectID": "chapter12.html#summary-statistics-1",
    "href": "chapter12.html#summary-statistics-1",
    "title": "8  Strategies for Variable Selection",
    "section": "10.1 Summary Statistics",
    "text": "10.1 Summary Statistics\nWe begin by summarizing the data.\n\nsummary(case1202)\n\n      Bsal          Sal77           Sex         Senior          Age           Educ     \n Min.   :3900   Min.   : 7860   Female:61   Min.   :65.0   Min.   :280   Min.   : 8.0  \n 1st Qu.:4980   1st Qu.: 9000   Male  :32   1st Qu.:74.0   1st Qu.:349   1st Qu.:12.0  \n Median :5400   Median :10020               Median :84.0   Median :468   Median :12.0  \n Mean   :5420   Mean   :10393               Mean   :82.3   Mean   :474   Mean   :12.5  \n 3rd Qu.:6000   3rd Qu.:11220               3rd Qu.:90.0   3rd Qu.:590   3rd Qu.:15.0  \n Max.   :8100   Max.   :16320               Max.   :98.0   Max.   :774   Max.   :16.0  \n     Exper      \n Min.   :  0.0  \n 1st Qu.: 35.5  \n Median : 70.0  \n Mean   :100.9  \n 3rd Qu.:144.0  \n Max.   :381.0  \n\n\nThe data is shown on page 350-351 as display 12.3. A total of 93 employee salaries are included: 61 females and 32 males. Next, we present a full graphical display for the variables within the data set and the log of the beginning salary variable.\n\nscatterplotMatrix(~log(Bsal) + Senior + Age + Educ + Exper | Sex, \n                  diagonal=list(method =\"histogram\"), \n                  col = carPalette(),\n                  smooth = FALSE, \n                  data = case1202)\n\n\n\n\n\n\n\n\nThrough these scatterplots it appears that beginning salary should be on the log scale and the starting model without the effects of gender will be a saturated second-order model variables including Seniority, Age, Education, Experience, as main effects, quadratic terms, and their full interactions (See Display 12.10 on page 367). The code below runs all subset model selection on this model. Note that the typical model conventions (having the main effect for a variable if it’s quadratic or interaction term are included) are not followed, so you need to think about these results.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Strategies for Variable Selection</span>"
    ]
  },
  {
    "objectID": "chapter12.html#model-selection",
    "href": "chapter12.html#model-selection",
    "title": "8  Strategies for Variable Selection",
    "section": "10.2 Model Selection",
    "text": "10.2 Model Selection\nTo determine the best subset of these variables we compare \\(C_p\\) and BIC statistics. Display 12.11 shows the \\(C_p\\) statistics for models that meet ‘good practice’ and have small \\(C_p\\) values. We will demonstrate how to calculate the \\(C_p\\) statistics for the two models with the lowest \\(C_p\\) statistics discussed in “Identifying Good Subset Models” on pages 367-368.\n\ncase1202_subsets &lt;- regsubsets(log(Bsal) ~ (Senior + Age + Educ + Exper)^2 + I(Senior^2) + \n                                 I(Age^2) + I(Age^2) + I(Exper^2), \n           nvmax=25, \n           data=case1202)\ncase1202_summary &lt;- summary(case1202_subsets)\n\nThe results can be a bit tough to digest, so it can be useful to make an index plot of the metric of interest.\n\np &lt;- apply(case1202_summary$which, 1, sum) # number of coefs in models\nindex_df &lt;- data.frame(p = p - 1, bic = case1202_summary$bic, \n                       cp = case1202_summary$cp)\n\nbic_plot &lt;- gf_point(bic ~ p, data = index_df)\ncp_plot &lt;- gf_point(cp ~ p, data = index_df)\ngrid.arrange(bic_plot, cp_plot, ncol = 2)\n\n\n\n\n\n\n\n\nUsing both BIC a model with 5 slope coefficients is flagged as “best” and using \\(C_p\\) a model with 6 slope coefficients is flagged as “best”. We can see the terms and their estimates by using the coef() command:\n\ncoef(case1202_subsets, id = 5)\n\n(Intercept)        Educ       Exper Senior:Educ   Age:Exper  Educ:Exper \n   8.07e+00    5.92e-02    4.53e-03   -2.78e-04   -3.65e-06   -1.42e-04 \n\ncoef(case1202_subsets, id = 6)\n\n(Intercept)        Educ       Exper    I(Age^2) Senior:Educ    Age:Educ   Age:Exper \n   7.90e+00    8.95e-02    3.82e-03    1.27e-06   -2.48e-04   -9.54e-05   -5.32e-06 \n\n\nLooking at the coefficients we see that these models do not follow model-fitting convention. For example, Senior:Educ is in the model without Senior. If you wish to use a method that follows this convention, then using a sequential model selection approach can be considered.\n\nssom &lt;- lm(log(Bsal) ~ (Senior + Age + Educ + Exper)^2 + I(Senior^2) + I(Age^2) + \n             I(Exper^2), data = case1202)\ncase1202_step &lt;- stepAIC(ssom, direction = \"backward\", k = log(nrow(case1202)), trace = 0)\nsummary(case1202_step)\n\n\nCall:\nlm(formula = log(Bsal) ~ Senior + Age + Educ + Exper + Age:Educ + \n    Age:Exper, data = case1202)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2817 -0.0476  0.0132  0.0605  0.2341 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  7.89e+00   2.45e-01   32.21  &lt; 2e-16\nSenior      -3.15e-03   1.04e-03   -3.04  0.00313\nAge          1.24e-03   4.02e-04    3.09  0.00270\nEduc         7.20e-02   1.67e-02    4.31  4.3e-05\nExper        2.86e-03   6.67e-04    4.28  4.8e-05\nAge:Educ    -1.02e-04   3.15e-05   -3.25  0.00166\nAge:Exper   -3.72e-06   1.02e-06   -3.65  0.00044\n\nResidual standard error: 0.0974 on 86 degrees of freedom\nMultiple R-squared:  0.469, Adjusted R-squared:  0.431 \nF-statistic: 12.6 on 6 and 86 DF,  p-value: 3.58e-10\n\n\nThe final model selected by backwards elimination matches the one proposed by Sleuth. To finish their analysis, the authors add Sex into the model:\n\ndiscrim_mod &lt;- update(case1202_step, . ~ . + Sex)\nsummary(discrim_mod)\n\n\nCall:\nlm(formula = log(Bsal) ~ Senior + Age + Educ + Exper + Sex + \n    Age:Educ + Age:Exper, data = case1202)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.17822 -0.05197 -0.00203  0.05301  0.20466 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  8.16e+00   2.21e-01   36.99  &lt; 2e-16\nSenior      -3.48e-03   9.09e-04   -3.83  0.00024\nAge          9.15e-04   3.57e-04    2.56  0.01218\nEduc         4.23e-02   1.57e-02    2.70  0.00836\nExper        2.18e-03   5.98e-04    3.65  0.00045\nSexMale      1.20e-01   2.29e-02    5.22  1.3e-06\nAge:Educ    -5.46e-05   2.91e-05   -1.88  0.06402\nAge:Exper   -3.23e-06   8.96e-07   -3.61  0.00052\n\nResidual standard error: 0.0853 on 85 degrees of freedom\nMultiple R-squared:  0.598, Adjusted R-squared:  0.564 \nF-statistic:   18 on 7 and 85 DF,  p-value: 1.79e-14\n\n\nNote: If you use regsubsets() for selection, you have to refit the selected model using lm() as usual in order to make predictions, etc.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Strategies for Variable Selection</span>"
    ]
  },
  {
    "objectID": "chapter20.html",
    "href": "chapter20.html",
    "title": "9  Logistic Regression for Binary Response Variables",
    "section": "",
    "text": "10 Survival in the Donner Party\nWe will also set some options to improve legibility of graphs and output.\nFor any given age, were the odds of survival in the Donner Party greater for women than men? This is the question addressed in case study 20.1 in the Sleuth.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Logistic Regression for Binary Response Variables</span>"
    ]
  },
  {
    "objectID": "chapter20.html#summary-statistics",
    "href": "chapter20.html#summary-statistics",
    "title": "9  Logistic Regression for Binary Response Variables",
    "section": "10.1 Summary statistics",
    "text": "10.1 Summary statistics\nWe begin by reading the data (which is done when you loaded the Sleuth3 package) and exploring the relationships between age, sex, and survival status.\n\nsummary(case2001)\n\n      Age           Sex          Status  \n Min.   :15.0   Female:15   Died    :25  \n 1st Qu.:24.0   Male  :30   Survived:20  \n Median :28.0                            \n Mean   :31.8                            \n 3rd Qu.:40.0                            \n Max.   :65.0                            \n\n\nTo begin exploring the relationship between Status and Age we can construct side-by-side boxplots\n\ngf_boxplot(Age ~ Status, data = case2001) %&gt;%\n  gf_refine(coord_flip())\n\n\n\n\n\n\n\n\nWe see that younger adults were more likely to survive. We can dig a little deeper into this association by creating a conditional density plot, where the conditional distribution of survival status given age is plotted:\n\ncdplot(Status ~ Age, data = case2001)\n\n\n\n\n\n\n\n\nThe conditional density plot supports the claim that younger adults seem more likely to survive.\nTo explore the relationship between Status and Sex we can construct a spine plot, where the conditional distribution of survival is displayed by sex.\n\nspineplot(Status ~ Sex, data = case2001)\n\n\n\n\n\n\n\n\nNote: In order for our results to match those presented in the Sleuth we must reorder the levels of Status, making Male the reference/baseline level. This is easily done using the relevel() command:\n\ncase2001$Sex &lt;- relevel(case2001$Sex, ref = \"Male\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Logistic Regression for Binary Response Variables</span>"
    ]
  },
  {
    "objectID": "chapter20.html#fitting-the-logistic-regression-model",
    "href": "chapter20.html#fitting-the-logistic-regression-model",
    "title": "9  Logistic Regression for Binary Response Variables",
    "section": "10.2 Fitting the logistic regression model",
    "text": "10.2 Fitting the logistic regression model\nThe following code presents the results interpreted on page 608 and presented in Display 20.6 (on page 614):\n\n# Parallel lines model\ndonner_mod1 &lt;- glm(Status ~ Age + Sex, data = case2001, family = binomial) \nsummary(donner_mod1)\n\n\nCall:\nglm(formula = Status ~ Age + Sex, family = binomial, data = case2001)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)   1.6331     1.1102    1.47    0.141\nAge          -0.0782     0.0373   -2.10    0.036\nSexFemale     1.5973     0.7555    2.11    0.034\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 61.827  on 44  degrees of freedom\nResidual deviance: 51.256  on 42  degrees of freedom\nAIC: 57.26\n\nNumber of Fisher Scoring iterations: 4\n\n\nA logistic regression model with an interaction term between age and sex is considered in Display 20.5 (on page 613). This model is fit using the code below:\n\n# Separate lines model\ndonner_mod2 &lt;- glm(Status ~ Age * Sex, data = case2001, family = binomial)\nsummary(donner_mod2)\n\n\nCall:\nglm(formula = Status ~ Age * Sex, family = binomial, data = case2001)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)     0.3183     1.1310    0.28    0.778\nAge            -0.0325     0.0353   -0.92    0.357\nSexFemale       6.9280     3.3989    2.04    0.042\nAge:SexFemale  -0.1616     0.0943   -1.71    0.086\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 61.827  on 44  degrees of freedom\nResidual deviance: 47.346  on 41  degrees of freedom\nAIC: 55.35\n\nNumber of Fisher Scoring iterations: 5",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Logistic Regression for Binary Response Variables</span>"
    ]
  },
  {
    "objectID": "chapter20.html#inferential-tools",
    "href": "chapter20.html#inferential-tools",
    "title": "9  Logistic Regression for Binary Response Variables",
    "section": "10.3 Inferential tools",
    "text": "10.3 Inferential tools\nFirst, Display 20.5 (on page 613) details how to carryout Wald’s test for \\(\\beta_{\\rm age \\times sex} = 0\\) from the second model. Notice that you can use the output from summary(donner_mod2) to conduct this test.\nNext, the Sleuth builds a confidence interval for \\(\\beta_{\\rm fem}\\) from the parallel lines model (i.e. model 1). This is not the confidence interval returned by confint(), so it must be constructed by hand. Remember that the model coefficients are on the log-odds scale, be sure to back-transform in order to obtain interpretations on the odds scale!\n\nbeta_fem &lt;- coef(donner_mod1)[3]\nse_fem &lt;- sqrt(vcov(donner_mod1)[3,3])\nupper &lt;- beta_fem + qnorm(.975) * se_fem\nupper\n\nSexFemale \n    3.078 \n\nlower &lt;- beta_fem - qnorm(.975) * se_fem\nlower\n\nSexFemale \n   0.1166 \n\nexp(upper)\n\nSexFemale \n    21.71 \n\nexp(lower)\n\nSexFemale \n    1.124 \n\n\nA more-reliable confidence interval for the coefficients can be obtained using the theory of the drop-in deviance test. This profile-likelihood confidence interval is obtained for all of the coefficients below:\n\nCIs &lt;- confint(donner_mod1, level = 0.95)\n\nWaiting for profiling to be done...\n\nCIs\n\n              2.5 %   97.5 %\n(Intercept) -0.4088  4.02474\nAge         -0.1624 -0.01407\nSexFemale    0.1951  3.22867\n\n# Backtransform to compare to the interval above\nexp(CIs[3,]) \n\n 2.5 % 97.5 % \n 1.215 25.246 \n\n\nNotice that the intervals may disagree. Here the normal distribution gives the interval (1.124, 21.71) while the profile-likelihood confidence interval is (1.215, 25.246).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Logistic Regression for Binary Response Variables</span>"
    ]
  },
  {
    "objectID": "chapter20.html#summary-statistics-1",
    "href": "chapter20.html#summary-statistics-1",
    "title": "9  Logistic Regression for Binary Response Variables",
    "section": "11.1 Summary statistics",
    "text": "11.1 Summary statistics\nWe begin by reading the data and recoding a few variable so that they match the coding using in the Sleuth.\n\nsummary(case2002)\n\n          LC          FM         SS           BK           AG           YR      \n LungCancer:49   Female: 36   High: 45   Bird  :67   Min.   :37   Min.   : 0.0  \n NoCancer  :98   Male  :111   Low :102   NoBird:80   1st Qu.:52   1st Qu.:20.0  \n                                                     Median :59   Median :30.0  \n                                                     Mean   :57   Mean   :27.9  \n                                                     3rd Qu.:63   3rd Qu.:39.0  \n                                                     Max.   :67   Max.   :50.0  \n       CD      \n Min.   : 0.0  \n 1st Qu.:10.0  \n Median :15.0  \n Mean   :15.7  \n 3rd Qu.:20.0  \n Max.   :45.0  \n\n# Adjust the coding to match Sleuth's\ncase2002 &lt;- case2002 %&gt;%\n  mutate(LC = relevel(LC, ref = \"NoCancer\"),\n         FM = relevel(FM, ref = \"Male\"),\n         SS = relevel(SS, ref = \"Low\"),\n         BK = relevel(BK, ref = \"NoBird\"))\n\nNext, we create a coded (and faceted) scatterplot of years of smoking against age, where the plotting symbols are used to represent lung cancer status (Display 20.10, page 621):\n\ngf_point(YR ~ AG | BK:LC, shape = ~LC, data = case2002) %&gt;%\n  gf_refine(theme(legend.position = \"none\"))",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Logistic Regression for Binary Response Variables</span>"
    ]
  },
  {
    "objectID": "chapter20.html#the-drop-in-deviance-test",
    "href": "chapter20.html#the-drop-in-deviance-test",
    "title": "9  Logistic Regression for Binary Response Variables",
    "section": "11.2 The drop-in-deviance test",
    "text": "11.2 The drop-in-deviance test\nThe goal of this case study is to see whether birdkeeping is associated with increased odds of lung cancer after accounting for several other factors. To do this, the Sleuth conducts a likelihood ratio test, also referred to as a drop-in-deviance test. In order conduct this test we need a full and reduced model, just like for an extra-sum-of-squares \\(F\\)-test for regression.\nThe full model from Display 20.7 (on page 616) is fitted below:\n\nfull_model &lt;- glm(LC ~ FM + AG + SS + YR + BK, data = case2002, family = binomial)\nfull_model\n\n\nCall:  glm(formula = LC ~ FM + AG + SS + YR + BK, family = binomial, \n    data = case2002)\n\nCoefficients:\n(Intercept)     FMFemale           AG       SSHigh           YR       BKBird  \n    -1.4062       0.5213      -0.0463       0.1321       0.0829       1.3349  \n\nDegrees of Freedom: 146 Total (i.e. Null);  141 Residual\nNull Deviance:      187 \nResidual Deviance: 155  AIC: 167\n\n\nThe reduced model from Display 20.7 (on page 616) is fitted below:\n\nreduced_model &lt;- glm(LC ~ FM + AG + SS + YR, data = case2002, family = binomial)\nreduced_model\n\n\nCall:  glm(formula = LC ~ FM + AG + SS + YR, family = binomial, data = case2002)\n\nCoefficients:\n(Intercept)     FMFemale           AG       SSHigh           YR  \n     0.1032       0.7214      -0.0632      -0.0564       0.0873  \n\nDegrees of Freedom: 146 Total (i.e. Null);  142 Residual\nNull Deviance:      187 \nResidual Deviance: 167  AIC: 177\n\n\nThe drop-in-deviance test can be conducted by hand, as shown below:\n\n# test statistic\ndd_stat &lt;- deviance(reduced_model) - deviance(full_model)\ndd_stat\n\n[1] 11.29\n\n# d.f.\ndf &lt;- df.residual(reduced_model) - df.residual(full_model)\ndf\n\n[1] 1\n\n# p-value\n1 - pchisq(dd_stat, df = 1)\n\n[1] 0.0007786\n\n\nAlternatively, it can be conducted using the anova() command:\n\nanova(reduced_model, full_model, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: LC ~ FM + AG + SS + YR\nModel 2: LC ~ FM + AG + SS + YR + BK\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1       142        166                     \n2       141        155  1     11.3  0.00078",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Logistic Regression for Binary Response Variables</span>"
    ]
  },
  {
    "objectID": "chapter20.html#examining-linearity",
    "href": "chapter20.html#examining-linearity",
    "title": "9  Logistic Regression for Binary Response Variables",
    "section": "11.3 Examining linearity",
    "text": "11.3 Examining linearity\nAs mentioned in passing on page 620, we can check whether the relationship between the logit and a quantitative predictor is linear by using the sample (empirical) logit. The setup code-chunk in this r markdown document defines the function elogit() which calculates the empirical logit and produces a data frame for plotting. You can copy and paste this code, or load it using the following code:\n\nsource(\"http://aloy.rbind.io/r/emplogit.R\")\n\nOnce you have this function, you can reproduce Display 20.11:\n\n## NEED TO FIX!!\nsample_logit &lt;- elogit(case2002$YR, case2002$LC, binsize = #breaks = c(0, 1, 20, 30, 40, 50))\ngf_point(elogit ~ midpoint.x, data = sample_logit) %&gt;%\n  gf_labs(x = \"Years smoked (midpoint of interval)\", y = \"Sample Logit\")\n\nNote: if you are not sure what intervals to use, but know the number of intervals desired, you can specify nbins rather than breaks in the elogit() function.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Logistic Regression for Binary Response Variables</span>"
    ]
  },
  {
    "objectID": "chapter21.html",
    "href": "chapter21.html",
    "title": "10  Logistic Regression for Binomial Counts",
    "section": "",
    "text": "11 Island Size and Bird Extinctions\nIn this chapter we need to load the following packages (remember, you will need to install packages you have never used before if you are using your own computer).\nWe will also set some options to improve legibility of graphs and output.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression for Binomial Counts</span>"
    ]
  },
  {
    "objectID": "chapter21.html#eda",
    "href": "chapter21.html#eda",
    "title": "10  Logistic Regression for Binomial Counts",
    "section": "11.1 EDA",
    "text": "11.1 EDA\nCalculating the proportions and empirical logits…\n\ncase2101 &lt;- case2101 %&gt;%\n  mutate(prop = Extinct / AtRisk, \n         logit = log((Extinct + 0.5) / (AtRisk - Extinct + 0.5)))\nhead(case2101)\n\n          Island  Area AtRisk Extinct    prop  logit\n1     Ulkokrunni 185.8     75       5 0.06667 -2.551\n2      Maakrunni 105.8     67       3 0.04478 -2.914\n3      Ristikari  30.7     66      10 0.15152 -1.683\n4 Isonkivenletto   8.5     51       6 0.11765 -1.946\n5 Hietakraasukka   4.8     28       3 0.10714 -1.986\n6      Kraasukka   4.5     20       4 0.20000 -1.299\n\n\n\ngf_point(logit ~ Area, data = case2101) %&gt;%\n  gf_refine(coord_trans(x=\"log10\")) %&gt;%\n  gf_labs(x = \"Area (sq. km, log scale)\", y = \"Logit\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression for Binomial Counts</span>"
    ]
  },
  {
    "objectID": "chapter21.html#fitting-a-logistic-regression-model",
    "href": "chapter21.html#fitting-a-logistic-regression-model",
    "title": "10  Logistic Regression for Binomial Counts",
    "section": "11.2 Fitting a logistic regression model",
    "text": "11.2 Fitting a logistic regression model\n\nisland_glm1 &lt;- glm(prop ~ log(Area), data = case2101,\n                   family = binomial, weights = AtRisk)\nsummary(island_glm1)\n\n\nCall:\nglm(formula = prop ~ log(Area), family = binomial, data = case2101, \n    weights = AtRisk)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  -1.1962     0.1184  -10.10  &lt; 2e-16\nlog(Area)    -0.2971     0.0549   -5.42  6.1e-08\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 45.338  on 17  degrees of freedom\nResidual deviance: 12.062  on 16  degrees of freedom\nAIC: 75.39\n\nNumber of Fisher Scoring iterations: 4",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression for Binomial Counts</span>"
    ]
  },
  {
    "objectID": "chapter21.html#examining-residuals",
    "href": "chapter21.html#examining-residuals",
    "title": "10  Logistic Regression for Binomial Counts",
    "section": "11.3 Examining residuals",
    "text": "11.3 Examining residuals\n\nisland_resids &lt;- case2101 %&gt;%\n  select(Island, prop) %&gt;%\n  mutate(raw.resid = resid(island_glm1, type = \"response\"),\n         pearson.resid = resid(island_glm1, type = \"pearson\"),\n         dev.resid = resid(island_glm1, type = \"deviance\"))\nhead(island_resids)\n\n          Island    prop raw.resid pearson.resid dev.resid\n1     Ulkokrunni 0.06667  0.006493        0.2365    0.2327\n2      Maakrunni 0.04478 -0.025585       -0.8188   -0.8737\n3      Ristikari 0.15152  0.052975        1.4440    1.3496\n4 Isonkivenletto 0.11765 -0.020351       -0.4214   -0.4307\n5 Hietakraasukka 0.10714 -0.052319       -0.7562   -0.7958\n6      Kraasukka 0.20000  0.037951        0.4606    0.4475",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression for Binomial Counts</span>"
    ]
  },
  {
    "objectID": "chapter21.html#deviance-goodness-of-fit-test",
    "href": "chapter21.html#deviance-goodness-of-fit-test",
    "title": "10  Logistic Regression for Binomial Counts",
    "section": "11.4 Deviance goodness-of-fit test",
    "text": "11.4 Deviance goodness-of-fit test\n\ndeviance(island_glm1)\n\n[1] 12.06\n\ndf.residual(island_glm1)\n\n[1] 16\n\n1 - pchisq(12.06, df = 16)\n\n[1] 0.7398\n\n\nNote: the deviance statistic can also be found are the “Residual deviance” in the summary of the fitted logistic regression model. (You should look at the summary above to verify this.)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression for Binomial Counts</span>"
    ]
  },
  {
    "objectID": "chapter21.html#inference-about-model-coefficients",
    "href": "chapter21.html#inference-about-model-coefficients",
    "title": "10  Logistic Regression for Binomial Counts",
    "section": "11.5 Inference about model coefficients",
    "text": "11.5 Inference about model coefficients\n\ntidy(island_glm1)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -1.20     0.118     -10.1  5.58e-24\n2 log(Area)     -0.297    0.0549     -5.42 6.08e- 8\n\n\n\nbeta1 &lt;- coef(island_glm1)[2]\nbeta1\n\nlog(Area) \n  -0.2971 \n\nse &lt;- sqrt(vcov(island_glm1)[2,2])\nse\n\n[1] 0.05485\n\nbeta1 + c(-1, 1) * qnorm(.975) * se\n\n[1] -0.4046 -0.1896\n\n\nDrop-in-deviance test\n\nisland_reduced &lt;- update(island_glm1, . ~ . - log(Area))\nanova(island_reduced, island_glm1, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: prop ~ 1\nModel 2: prop ~ log(Area)\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1        17       45.3                     \n2        16       12.1  1     33.3    8e-09",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression for Binomial Counts</span>"
    ]
  },
  {
    "objectID": "chapter21.html#eda-1",
    "href": "chapter21.html#eda-1",
    "title": "10  Logistic Regression for Binomial Counts",
    "section": "12.1 EDA",
    "text": "12.1 EDA\nCalculating the proportions and empirical logits…\n\ncase2102 &lt;- case2102 %&gt;%\n  mutate(prop = Removed / Placed,\n         logit = log((Removed + 0.5) / (Placed - Removed + 0.5)))\nhead(case2102)\n\n  Morph Distance Placed Removed   prop   logit\n1 light      0.0     56      17 0.3036 -0.8141\n2  dark      0.0     56      14 0.2500 -1.0754\n3 light      7.2     80      28 0.3500 -0.6109\n4  dark      7.2     80      20 0.2500 -1.0822\n5 light     24.1     52      18 0.3462 -0.6232\n6  dark     24.1     52      22 0.4231 -0.3042\n\n\n\ngf_point(logit ~ Distance, data = case2102, color = ~Morph) %&gt;%\n  gf_line() %&gt;%\n  gf_refine(scale_color_brewer(palette = \"Set2\")) %&gt;%\n  gf_labs(x = \"Distance (km) from Liverpool\",\n          y = \"Logit\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression for Binomial Counts</span>"
    ]
  },
  {
    "objectID": "chapter22.html",
    "href": "chapter22.html",
    "title": "11  Poisson Regression for Counts",
    "section": "",
    "text": "12 Age and Mating Success of Male Elephants\nIn this chapter we need to load the following packages (remember, you will need to install packages you have never used before if you are using your own computer).\nWe will also set some options to improve legibility of graphs and output.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Poisson Regression for Counts</span>"
    ]
  },
  {
    "objectID": "chapter22.html#eda",
    "href": "chapter22.html#eda",
    "title": "11  Poisson Regression for Counts",
    "section": "12.1 EDA",
    "text": "12.1 EDA\nTo begin, we load the data and explore a few basic summary statistics:\n\nsummary(case2201)\n\n      Age          Matings    \n Min.   :27.0   Min.   :0.00  \n 1st Qu.:29.0   1st Qu.:1.00  \n Median :34.0   Median :2.00  \n Mean   :35.9   Mean   :2.68  \n 3rd Qu.:42.0   3rd Qu.:3.00  \n Max.   :52.0   Max.   :9.00  \n\n\nNext we plot the logarithm of the number of matings against age to explore the form of the relationship. (Note: 0.5 was added to Matings so that the natural log could be computed for all observations.) The plot reveals potential curvature in the relationship, which will be further explored.\n\ngf_jitter(log(Matings + 0.5) ~ Age, data = case2201, height = 0.25, width = 0.25, pch = 1) %&gt;%\n  gf_labs(x = \"Age (years) -- Slightly Jittered\", y = \"Number of Matings (log scale)\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Poisson Regression for Counts</span>"
    ]
  },
  {
    "objectID": "chapter22.html#fitting-the-poisson-regression-model",
    "href": "chapter22.html#fitting-the-poisson-regression-model",
    "title": "11  Poisson Regression for Counts",
    "section": "12.2 Fitting the Poisson regression model",
    "text": "12.2 Fitting the Poisson regression model\nNext, we fit the Poisson log-linear regression model with log-mean function\n\\[\\log(\\mu) = \\beta_0 + \\beta_1 {\\tt age} + \\beta_2 {\\tt age}^2\\]\n\nglm1 &lt;- glm(Matings ~ Age + I(Age^2), data = case2201, family = poisson)\nsummary(glm1)\n\n\nCall:\nglm(formula = Matings ~ Age + I(Age^2), family = poisson, data = case2201)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -2.85741    3.03564   -0.94     0.35\nAge          0.13595    0.15801    0.86     0.39\nI(Age^2)    -0.00086    0.00201   -0.43     0.67\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 75.372  on 40  degrees of freedom\nResidual deviance: 50.826  on 38  degrees of freedom\nAIC: 158.3\n\nNumber of Fisher Scoring iterations: 5\n\n\nAs pointed out by Sleuth (pp. 681-682), most of the estimated means are less than 5, so it is not wise to compare the residual deviance to a \\(\\chi^2\\) distribution.\n\nsummary(fitted(glm1))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.21    1.44    2.16    2.68    3.81    6.61",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Poisson Regression for Counts</span>"
    ]
  },
  {
    "objectID": "chapter22.html#inference",
    "href": "chapter22.html#inference",
    "title": "11  Poisson Regression for Counts",
    "section": "12.3 Inference",
    "text": "12.3 Inference\nWald test results are provided by summary() as part of the table of coefficients. We can also extract these results using the tidy() function from the broom package:\n\ntidy(glm1)\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -2.86       3.04       -0.941   0.347\n2 Age          0.136      0.158       0.860   0.390\n3 I(Age^2)    -0.000860   0.00201    -0.427   0.669\n\n\nFurther, we can construct confidence intervals either using the Wald procedure or the profile likelihood procedure. On page 683, this is discussed for the reduced model, without the quadratic term:\n\n# Eliminating the quadratic term\nreduced &lt;- update(glm1, . ~ . - I(Age^2))\n\n# Wald CI\nbeta1 &lt;- coef(reduced)[2]\nse &lt;- sqrt(vcov(reduced)[2,2])\nbeta1 + c(-1, 1) * qnorm(.975) * se\n\n[1] 0.04175 0.09563\n\nexp(beta1 + c(-1, 1) * qnorm(.975) * se)\n\n[1] 1.043 1.100\n\n# Profile-likelihood CIs\nconfint(glm1)\n\nWaiting for profiling to be done...\n\n\n                2.5 %   97.5 %\n(Intercept) -9.044944 2.893178\nAge         -0.163750 0.457734\nI(Age^2)    -0.004968 0.002948\n\n\nA drop-in-deviance test is carried out the same way it was in binomial logistic regression: we compare the full and reduced models using the anova() command.\n\nanova(reduced, glm1, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: Matings ~ Age\nModel 2: Matings ~ Age + I(Age^2)\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1        39       51.0                     \n2        38       50.8  1    0.185     0.67",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Poisson Regression for Counts</span>"
    ]
  },
  {
    "objectID": "chapter22.html#eda-1",
    "href": "chapter22.html#eda-1",
    "title": "11  Poisson Regression for Counts",
    "section": "13.1 EDA",
    "text": "13.1 EDA\nTo begin, we load the data and explore a few basic summary statistics:\n\nsummary(case2202)\n\n      Site       Salamanders       PctCover    ForestAge    \n Min.   : 1.0   Min.   : 0.00   Min.   : 0   Min.   :  0.0  \n 1st Qu.:12.5   1st Qu.: 0.00   1st Qu.:18   1st Qu.: 29.5  \n Median :24.0   Median : 1.00   Median :83   Median : 64.0  \n Mean   :24.0   Mean   : 2.47   Mean   :59   Mean   :168.8  \n 3rd Qu.:35.5   3rd Qu.: 3.00   3rd Qu.:88   3rd Qu.:266.5  \n Max.   :47.0   Max.   :13.00   Max.   :93   Max.   :675.0  \n\n\nNext we plot the logarithm of the number of salamanders against the potential explanatory variables.\n\nscatterplotMatrix(~ PctCover + ForestAge + log(Salamanders + 0.5), data = case2202, \n                  smooth = FALSE, var.labels = c(\"PctCover\", \"ForestAge\", \"Salamanders\"))\n\n\n\n\n\n\n\n\nThe interpretation and modeling implications of this scatterplot matrix are discussed on page 682. One key finding is that there appears to be defined groups based on the canopy cover: closed canopy (PctCover &gt; 70) and open canopy (PctCover &lt; 60). We add this grouping variable to the data set below.\n\n# Using 70% as the dividing line\ncase2202$Closed &lt;- ifelse(case2202$PctCover &gt; 70,\"closed\", \"open\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Poisson Regression for Counts</span>"
    ]
  },
  {
    "objectID": "chapter22.html#fitting-the-poisson-regression-model-1",
    "href": "chapter22.html#fitting-the-poisson-regression-model-1",
    "title": "11  Poisson Regression for Counts",
    "section": "13.2 Fitting the Poisson regression model",
    "text": "13.2 Fitting the Poisson regression model\nTo begin, a saturated second-order model (i.e. separate quadratic model) is fit for the log-mean outlined on page 682, and detailed on page 689.\n\nssom &lt;- glm(Salamanders ~ PctCover * ForestAge * Closed + I(PctCover^2) + I(ForestAge^2) + \n              I(PctCover^2):Closed + I(ForestAge^2):Closed, \n            data = case2202, family = poisson)\nsummary(ssom)\n\n\nCall:\nglm(formula = Salamanders ~ PctCover * ForestAge * Closed + I(PctCover^2) + \n    I(ForestAge^2) + I(PctCover^2):Closed + I(ForestAge^2):Closed, \n    family = poisson, data = case2202)\n\nCoefficients:\n                               Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                   -2.76e+02   6.45e+01   -4.27  1.9e-05\nPctCover                       6.50e+00   1.51e+00    4.32  1.6e-05\nForestAge                     -3.26e-02   2.17e-02   -1.50  0.13321\nClosedopen                     2.74e+02   6.45e+01    4.25  2.1e-05\nI(PctCover^2)                 -3.81e-02   8.80e-03   -4.33  1.5e-05\nI(ForestAge^2)                -3.42e-06   3.21e-06   -1.06  0.28719\nPctCover:ForestAge             4.00e-04   2.45e-04    1.64  0.10186\nPctCover:Closedopen           -6.45e+00   1.51e+00   -4.27  2.0e-05\nForestAge:Closedopen           7.93e-02   1.02e-01    0.77  0.43852\nClosedopen:I(PctCover^2)       3.63e-02   9.72e-03    3.73  0.00019\nClosedopen:I(ForestAge^2)     -2.58e-03   2.91e-03   -0.89  0.37534\nPctCover:ForestAge:Closedopen  2.41e-03   5.75e-03    0.42  0.67523\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 190.219  on 46  degrees of freedom\nResidual deviance:  89.178  on 35  degrees of freedom\nAIC: 198.2\n\nNumber of Fisher Scoring iterations: 6\n\n\nThe deviance goodness-of-fit test points to probable lack of fit, though this is only a rough test since the \\(\\chi^2\\) approximation based on large Poisson means is questionable.\n\n1 - pchisq(89.178, df = 35)\n\n[1] 1.281e-06\n\n\nA plot of the deviance residuals against the fitted means can be examined (Display 22.7, page 681):\n\ngf_point(residuals(ssom, type = \"deviance\") ~ fitted(ssom)) %&gt;%\n  gf_hline(yintercept = 0, color = \"blue\") %&gt;%\n  gf_hline(yintercept = 2, color = \"gray60\", linetype = 2)  %&gt;%\n  gf_hline(yintercept = -2, color = \"gray60\", linetype = 2) %&gt;%\n  gf_labs(x = \"Fitted means\", y = \"Deviance residual\")\n\n\n\n\n\n\n\n\nThe residuals do not point to a problem with one or two outliers, rather it appears that the variability in the response exceeds what is allowed for by the Poisson log-linear model.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Poisson Regression for Counts</span>"
    ]
  },
  {
    "objectID": "chapter22.html#model-building-using-quasi-likelihood",
    "href": "chapter22.html#model-building-using-quasi-likelihood",
    "title": "11  Poisson Regression for Counts",
    "section": "13.3 Model building using quasi-likelihood",
    "text": "13.3 Model building using quasi-likelihood\nAfter identifying over dispersion (i.e. extra-Poisson variation), Sleuth proceeds with model building using quasi-likelihood methods. To test whether forest age is an important predictor after accounting for the other variables, we use a drop-in-deviance F test (see display 22.10 on page 689). To do this, we must refit the separate quadratic (i.e. SSOM) model using quasilikelihood and fit the reduced model specified in the null hypothesis:\n\n# The full model via quasi-likelihood\nssom &lt;- update(ssom, . ~ ., family = quasipoisson)\n\n# The reduced (inferential) model\ninferential_model &lt;- glm(Salamanders ~ PctCover * Closed + I(PctCover^2) + \n                           I(PctCover^2):Closed, data = case2202, family = quasipoisson)\n\n# drop-in-deviance F test\nanova(inferential_model, ssom, test = \"F\")\n\nAnalysis of Deviance Table\n\nModel 1: Salamanders ~ PctCover * Closed + I(PctCover^2) + I(PctCover^2):Closed\nModel 2: Salamanders ~ PctCover + ForestAge + Closed + I(PctCover^2) + \n    I(ForestAge^2) + PctCover:ForestAge + PctCover:Closed + ForestAge:Closed + \n    Closed:I(PctCover^2) + Closed:I(ForestAge^2) + PctCover:ForestAge:Closed\n  Resid. Df Resid. Dev Df Deviance    F Pr(&gt;F)\n1        41       97.2                        \n2        35       89.2  6     8.05 0.57   0.75\n\n\nThere is no evidence that the terms including forest age are necessary predictors. The reduced model (from Display 22.4) is shown below:\n\naug &lt;- augment(inferential_model)\ngf_point(Salamanders ~ PctCover, data = aug) %&gt;%\n  gf_line(exp(.fitted) ~ PctCover, color = ~Closed) %&gt;%\n  gf_labs(x = \"Percentage of Canopy Cover\",\n          y = \"Salamander Count\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Poisson Regression for Counts</span>"
    ]
  }
]